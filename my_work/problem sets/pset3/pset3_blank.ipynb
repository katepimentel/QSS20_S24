{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem set 3: Text analysis of DOJ press releases\n",
    "\n",
    "**Total points (without extra credit)**: 52 \n",
    "\n",
    "- For background:\n",
    "\n",
    "    - DOJ is the federal law enforcement agency responsible for federal prosecutions; this contrasts with the local prosecutions in the Cook County dataset we analyzed earlier. Here's a short explainer on which crimes get prosecuted federally versus locally: https://www.criminaldefenselawyer.com/resources/criminal-defense/federal-crime/state-vs-federal-crimes.htm#:~:text=Federal%20criminal%20prosecutions%20are%20handled,of%20state%20and%20local%20law. \n",
    "    - Here's the Kaggle that contains the data: https://www.kaggle.com/jbencina/department-of-justice-20092018-press-releases \n",
    "    - Here's the code the dataset creator used to scrape those press releases here if you're interested: https://github.com/jbencina/dojreleases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.0 Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "## helpful packages\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "\n",
    "## nltk imports\n",
    "import nltk\n",
    "# nltk.download('vader_lexicon')\n",
    "from nltk.tokenize import word_tokenize\n",
    "### uncomment and run these lines if you haven't downloaded relevant nltk add-ons yet\n",
    "### nltk.download('averaged_perceptron_tagger')\n",
    "### nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "## spacy imports\n",
    "import spacy\n",
    "### uncomment and run the below line if you haven't loaded the en_core_web_sm library yet\n",
    "# ! python -m spacy download en_core_web_sm\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "## vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "## sentiment\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "## lda\n",
    "from gensim import corpora\n",
    "import gensim\n",
    "\n",
    "## repeated printouts and wide-format text\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 Load and clean text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>contents</th>\n",
       "      <th>date</th>\n",
       "      <th>topics_clean</th>\n",
       "      <th>components_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>Convicted Bomb Plotter Sentenced to 30 Years</td>\n",
       "      <td>PORTLAND, Oregon. – Mohamed Osman Mohamud, 23, who was convicted in 2013 of attempting to use a weapon of mass destruction (explosives) in connection with a plot to detonate a vehicle bomb at an annual Christmas tree lighting ceremony in Portland, was sentenced today to serve 30 years in prison, followed by a lifetime term of supervised release. Mohamud, a naturalized U.S. citizen from Somalia and former resident of Corvallis, Oregon, was arrested on Nov. 26, 2010, after he attempted to detonate what he believed to be an explosives-laden van that was parked near the tree lighting ceremony in Portland.  The arrest was the culmination of a long-term undercover operation, during which Mohamud was monitored closely for months as his bomb plot developed.  The device was in fact inert, and the public was never in danger from the device. At sentencing, United States District Court Judge Garr M. King, who presided over Mohamed’s 14-day trial, said “the intended crime was horrific,” and that the defendant, even though he was presented with options by undercover FBI employees, “never once expressed a change of heart.”  King further noted that the Christmas tree ceremony was attended by up to 10,000 people, and that the defendant “wanted everyone to leave either dead or injured.”  King said his sentence was necessary in view of the seriousness of the crime and to serve as deterrence to others who might consider similar acts.     “With today’s sentencing, Mohamed Osman Mohamud is being held accountable for his attempted use of what he believed to be a massive bomb to attack innocent civilians attending a public Christmas tree lighting ceremony in Portland,” said John P. Carlin, Assistant Attorney General for National Security.  “The evidence clearly indicated that Mohamud was intent on killing as many people as possible with his attack.  Fortunately, law enforcement was able to identify him as a threat, insert themselves in the place of a terrorist that Mohamud was trying to contact, and thwart Mohamud’s efforts to conduct an attack on our soil.  This case highlights how the use of undercover operations against would-be terrorists allows us to engage and disrupt those who wish to commit horrific acts of violence against the innocent public.  The many agents, analysts, and prosecutors who have worked on this case deserve great credit for their roles in protecting Portland from the threat posed by this defendant and ensuring that he was brought to justice.” “This trial provided a rare glimpse into the techniques Al Qaeda employs to radicalize home-grown extremists,” said Amanda Marshall, U.S. Attorney for the District of Oregon.  “With the sentencing today, the court has held this defendant accountable.   I thank the dedicated professionals in the law enforcement and intelligence communities who were responsible for this successful outcome.  I look forward to our continued work with Muslim communities in Oregon who are committed to ensuring that all young people are safe from extremists who seek to radicalize others to engage in violence.”  According to the trial evidence, in February 2009, Mohamud began communicating via e-mail with Samir Khan, a now-deceased al Qaeda terrorist who published Jihad Recollections, an online magazine that advocated violent jihad, and who also published Inspire, the official magazine of al-Qaeda in the Arabian Peninsula.  Between February and August 2009, Mohamed exchanged approximately 150 emails with Khan.  Mohamud wrote several articles for Jihad Recollections that were published under assumed names. In August 2009, Mohamud was in email contact with Amro Al-Ali, a Saudi national who was in Yemen at the time and is today in custody in Saudi Arabia for terrorism offenses.  Al-Ali sent Mohamud detailed e-mails designed to facilitate Mohamud’s travel to Yemen to train for violent jihad.  In December 2009, while Al-Ali was in the northwest frontier province of Pakistan, Mohamud and Al-Ali discussed the possibility of Mohamud traveling to Pakistan to join Al-Ali in terrorist activities. Mohamud responded to Al-Ali in an e-mail: “yes, that would be wonderful, just tell me what I need to do.”  Al-Ali referred Mohamud to a second associate overseas and provided Mohamud with a name and email address to facilitate the process. In the following months, Mohamud made several unsuccessful attempts to contact Al-Ali’s associate.  Ultimately, an FBI undercover operative contacted Mohamud via email under the guise of being an associate of Al-Ali’s.  Mohamud and the FBI undercover operative agreed to meet in Portland in July 2010.  At the meeting, Mohamud told the FBI undercover operative he had written articles that were published in Jihad Recollections.  Mohamud also said that he wanted to become “operational.”  Asked what he meant by “operational,” Mohamud said he wanted to put an explosion together, but needed help. According to evidence presented at trial, at a meeting in August 2010, Mohamud told undercover FBI operatives he had been thinking of committing violent jihad since the age of 15.  Mohamud then told the undercover FBI operatives that he had identified a potential target for a bomb: the annual Christmas tree lighting ceremony in Portland’s Pioneer Courthouse Square on Nov. 26, 2010.  The undercover FBI operatives cautioned Mohamud several times about the seriousness of this plan, noting there would be many people at the event, including children, and emphasized that Mohamud could abandon his attack plans at any time with no shame.  Mohamud indicated the deaths would be justified and that he would not mind carrying out a suicide attack on the crowd. According to evidence presented at trial, in the ensuing months Mohamud continued to express his interest in carrying out the attack and worked on logistics.  On Nov. 4, 2010, Mohamud and the undercover FBI operatives traveled to a remote location in Lincoln County, Oregon, where they detonated a bomb concealed in a backpack as a trial run for the upcoming attack.  During the drive back to Corvallis, Mohamud was asked if was capable looking at all the bodies of those who would be killed during the explosion.  In response, Mohamud noted, “I want whoever is attending that event to be, to leave either dead or injured.”  Mohamud later recorded a video of himself, with the assistance of the undercover FBI operatives, in which he read a statement that offered his rationale for his bomb attack.  On Nov. 18, 2010, undercover FBI operatives picked up Mohamud to travel to Portland to finalize the details of the attack.  On Nov. 26, 2010, just hours before the planned attack, Mohamud examined the 1,800 pound bomb in the van and remarked that it was “beautiful.”  Later that day, Mohamud was arrested after he attempted to remotely detonate the inert vehicle bomb rked near the Christmas tree lighting ceremony This case was investigated by the FBI, with assistance from the Oregon State Police, the Corvallis Police Department, the Lincoln County Sheriff’s Office and the Portland Police Bureau.  The prosecution was handled by Assistant U.S. Attorneys Ethan D. Knight and Pamala Holsinger from the U.S. Attorney’s Office for the District of Oregon.  Trial Attorney Jolie F. Zimmerman, from the Counterterrorism Section of the Justice Department’s National Security Division, assisted. # # # 14-1077</td>\n",
       "      <td>2014-10-01T00:00:00-04:00</td>\n",
       "      <td>No topic</td>\n",
       "      <td>National Security Division (NSD)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12-919</td>\n",
       "      <td>$1 Million in Restitution Payments Announced to Preserve North Carolina Wetlands</td>\n",
       "      <td>WASHINGTON – North Carolina’s Waccamaw River watershed will benefit from a $1 million restitution order from a federal court, funding environmental projects to acquire and preserve wetlands in an area damaged by illegal releases of wastewater from a corporate hog farm, announced Ignacia S. Moreno, Assistant Attorney General of the Justice Department’s Environment and Natural Resources Division; U.S. Attorney for the Eastern District of North Carolina Thomas G. Walker; Director Greg McLeod from the North Carolina State Bureau of Investigation; and Camilla M. Herlevich, Executive Director of the North Carolina Coastal Land Trust.   Freedman Farms Inc. was sentenced in February 2012 to five years of probation and ordered to pay $1.5 million in fines, restitution and community service payments for violating the Clean Water Act when it discharged hog waste into a stream that leads to the Waccamaw River.  William B. Freedman, president of Freedman Farms, was sentenced to six months in prison to be followed by six months of home confinement.  Freedman Farms also is required to implement a comprehensive environmental compliance program and institute an annual training program.   In an order issued on April 19, 2012, the court ordered that the defendants would be responsible for restitution of $1 million in the form of five annual payments starting in January 2013, which the court will direct to the North Carolina Coastal Land Trust (NCCLT).  The NCCLT plans to use the money to acquire and conserve land along streams in the Waccamaw watershed.  The court also directed a $75,000 community service payment to the Southern Environmental Enforcement Network, an organization dedicated to environmental law enforcement training and information sharing in the region.    “The resolution of the case against Freedman Farms demonstrates the commitment of the Department of Justice to enforcing the Clean Water Act to ensure the protection of human health and the environment,” said Assistant Attorney General Moreno.  “The court-ordered restitution in this case will conserve wetlands for the benefit of the people of North Carolina.  By enforcing the nation’s environmental laws, we will continue to ensure that concentrated animal feeding operations (CAFOs) operate without threatening our drinking water, the health of our communities and the environment.”   “This office is committed to doing our part to hold accountable those who commit crimes against our environment, which can cause serious health problems to residents and damage the environment that makes North Carolina such a beautiful place to live and visit,” said U.S. Attorney Walker.   “This case shows what we can accomplish when our SBI agents work closely with their local, state and federal partners to investigate environmental crimes and hold the polluters accountable,” said Director McLeod.  “We’ll continue our efforts to fight illegal pollution that damages our water and puts the public’s health at risk.”    “The Waccamaw is unique and wild,” said Director Herlevich of the North Carolina Coastal Land Trust. “Its watershed includes some of the most extensive cypress gum swamps in the state, and its headwaters at Lake Waccamaw contain fish that are found nowhere else on Earth.  We appreciate the trust of the court and the U. S. Attorney, and we look forward to using these funds for conservation projects in a river system that is one of our top conservation priorities.”   According to evidence presented in court, in December 2007 Freedman Farms discharged hog waste into Browder’s Branch, a tributary to the Waccamaw River that flows through the White Marsh, a large wetlands complex.  Freedman Farms, located in Columbus County, N.C., is in the business of raising hogs for market, and this particular farm had some 4,800 hogs.  The hog waste was supposed to be directed to two lagoons for treatment and disposal.  Instead, hog waste was discharged from Freedman Farms directly into Browder’s Branch.    The Clean Water Act is a federal law that makes it illegal to knowingly or negligently discharge a pollutant into a water of the United States.    The Freedman case was investigated by the U.S. Environmental Protection Agency (EPA) Criminal Investigation Division, the U.S. Army Corps of Engineers and the North Carolina State Bureau of Investigation, with assistance from the EPA Science and Ecosystem Support Division.  The case was prosecuted by Assistant U.S. Attorney J. Gaston B. Williams of the Eastern District of North Carolina and Trial Attorney Mary Dee Carraway of the Environmental Crimes Section of the Justice Department’s Environment and Natural Resources Division.   The North Carolina Coastal Land Trust is celebrating its 20th anniversary of saving special lands in eastern North Carolina. The organization has protected nearly 50,000 acres of lands with scenic, recreational, historic and ecological values. North Carolina Coastal Land Trust has saved streams and wetlands that provide clean water, forests that are havens for wildlife, working farms that provide local food and nature parks that everyone can enjoy.  More information about the Coastal Land Trust is available at www.coastallandtrust.org.</td>\n",
       "      <td>2012-07-25T00:00:00-04:00</td>\n",
       "      <td>No topic</td>\n",
       "      <td>Environment and Natural Resources Division</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11-1002</td>\n",
       "      <td>$1 Million Settlement Reached for Natural Resource Damages at Superfund Site in Massachusetts</td>\n",
       "      <td>BOSTON– A $1-million settlement has been reached for natural resource damages (NRD) at the Blackburn &amp; Union Privileges Superfund Site in Walpole, Mass., the Departments of Justice and Interior (DOI), and the Office of the Massachusetts Attorney General announced today.                The Blackburn &amp; Union Privileges Superfund Site includes 22 acres of contaminated land and water in Walpole. The contamination resulted from the operations of various industrial facilities dating back to the 19th century that exposed the site to asbestos, arsenic, lead and other hazardous substances.                The private parties involved in the settlement include two former owners and operators of the site, W.R. Grace &amp; Co.– Conn. and Tyco Healthcare Group LP, as well as the current owners, BIM Investment Corp. and Shaffer Realty Nominee Trust.               From about 1915 to 1936, a predecessor of W.R. Grace manufactured asbestos brake linings and clutch linings on a large portion of the property. From 1946 to about 1983, a predecessor of Tyco Healthcare operated a cotton fabric manufacturing business, which used caustic solutions, on a portion of the property.               In a 2010 settlement with U.S. Environmental Protection Agency (EPA), the four private parties agreed to perform a remedial action to clean up the site at an estimated cost of $13 million. The consent decree lodged today resolves both state and federal NRD liability claims; it requires the parties to pay $1,094,169.56 to the state and federal natural resource trustees, the Massachusetts Executive Office of Energy and Environmental Affairs (EEA) and DOI, for injuries to ecological resources including groundwater and wetlands, which provide habitat for waterfowl and wading birds, including black ducks and great blue herons.  The trustees will use the settlement funds for natural resource restoration projects in the area.               “This settlement demonstrates our commitment to recovering damages from the parties responsible for injury to natural resources, in partnership with state trustees,” said Bruce Gelber, Acting Deputy Assistant Attorney General of the Justice Department’s Environment and Natural Resources Division.               “The citizens of Walpole have had to live with the environmental impact of this contamination for many years,” Attorney General Martha Coakley said. “We are pleased that today’s agreement will not only require the responsible parties to reimburse taxpayer dollars, but will also provide funding to begin restoring or replacing the wetland and other natural resources.”                 The consent decree was lodged in the U.S. District Court for Massachusetts.     A portion of the funds, $300,000, will be distributed to the EEA-sponsored groundwater restoration projects; $575,000 will be used for ecological restoration projects jointly sponsored by EEA and the U.S. Fish and Wildlife Service (FWS).               In addition, $125,000 will go for projects jointly sponsored by EEA and FWS that achieve both ecological and groundwater restoration; $57,491.34 will be allocated for reimbursement for the FWS’s assessment costs; and $36,678.22 will be distributed as reimbursement for the commonwealth’s assessment costs.       “This settlement provides the means for a range of projects designed to compensate the public for decades of groundwater and other ecological damage at this site.  I encourage local citizens and organizations to become engaged in the public process that will take place as we solicit, take comment on, and choose these projects in the months ahead,” said Energy and Environmental Affairs Secretary Richard K. Sullivan Jr., who serves as the Commonwealth’s Natural Resources Damages trustee.       “This settlement will help restore habitat for fish and wildlife in the Neponset River watershed,” said Tom Chapman of the FWS New England Field Office. “We look forward to working with the commonwealth and local stakeholders to implement restoration.”               “More than 100 years-worth of industrial activities at this site caused major environmental contamination to the Neponset River, nearby wetlands and to groundwater below the site,” said Commissioner Kenneth Kimmell of the Massachusetts Department of Environmental Protection (MassDEP), which will staff the Trustee Council for the Commonwealth. “We will ensure that the community and the public will be active participants in the process to use these NRD funds to restore the injured natural resources.”                Under the federal Comprehensive Environmental Response, Compensation and Liability Act, EEA and DOI, acting through the FWS, are the designated state and federal natural resource Trustees for the site. The site has been listed on the EPA’s National Priorities List since 1994.        The consent decree is subject to a public comment period and court approval. A copy of the consent decree and instructions about how to submit comments is available on www.usdoj.gov/enrd/Consent_Decrees.html  .               After the consent decree is approved, EEA and FWS will develop proposed restoration plans to use the settlement funds for restoration projects. The proposed restoration plans will also be made available to the public for review and comment.                Assistant Attorney General Matthew Brock of Massachusetts Attorney General Coakley's Environmental Protection Division handled this matter.  Attorney Jennifer Davis of MassDEP, Attorney Anna Blumkin of EEA and MassDEP’s NRD Coordinator Karen Pelto also worked on this settlement.</td>\n",
       "      <td>2011-08-03T00:00:00-04:00</td>\n",
       "      <td>No topic</td>\n",
       "      <td>Environment and Natural Resources Division</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10-015</td>\n",
       "      <td>10 Las Vegas Men Indicted \\r\\nfor Falsifying Vehicle Emissions Tests</td>\n",
       "      <td>WASHINGTON—A federal grand jury in Las Vegas today returned indictments against 10 Nevada-certified emissions testers for falsifying vehicle emissions test reports, the Justice Department announced.   Each defendant faces one felony Clean Air Act count for falsifying reports between November 2007 and May 2009. The number of falsifications varied by defendant, with some defendants having falsified approximately 250 records, while others falsified more than double that figure. One defendant is alleged to have falsified over 700 reports.   The individuals indicted include:     Escudero resides in Pahrump, Nev. All other individuals are from Clark County, Nev.    The 10 defendants are alleged to have engaged in a practice known as \"clean scanning\" vehicles. The scheme involved entering the Vehicle Identification Number (VIN) for a vehicle that would not pass the emissions test into the computerized system, then connecting a different vehicle the testers knew would pass the test. These falsifications were allegedly performed for anywhere from $10 to $100 over and above the usual emissions testing fee.    The U.S. Environmental Protection Agency (EPA), under the Clean Air Act, requires the state of Nevada to conduct vehicle emissions testing in certain areas because the areas exceed national standards for carbon monoxide and ozone. Las Vegas is currently required to perform emissions testing.    To obtain a registration renewal, vehicle owners bring the vehicles to a licensed inspection station for testing. The emissions inspector logs into a computer to activate the system by using a unique password issued to the emissions inspector. The emissions inspector manually inputs the vehicle’s VIN to identify the tested vehicle, then connects the vehicle for model year 1996 and later to an onboard diagnostics port connected to an analyzer. The analyzer downloads data from the vehicle’s computer, analyzes the data and provides a \"pass\" or \"fail\" result. The pass or fail result and vehicle identification data are reported on the Vehicle Inspection Report. It is a crime to knowingly alter or conceal any record or other document required to be maintained by the Clean Air Act.     \"Falsifications of vehicle emissions testing, such as those alleged in the indictments unsealed today, are serious matters and we intend to use all of our enforcement tools to stop this harmful practice. These actions undermine a system that is designed to reduce air pollutants including smog and provide better air quality for the citizens of Nevada,\" said Ignacia S. Moreno, Assistant Attorney General for the Justice Department’s Environment and Natural Resources Division.    \"The residents of Nevada deserve to know that the vast majority of licensed vehicle emission inspectors are not corrupt and are not circumventing emission testing procedures,\" said U.S. Attorney Bogden. \"These indictments should serve as a clear warning to offenders that the Department of Justice will prosecute you if you make fraudulent statements and reports concerning compliance with the federal Clean Air Act.\"    \"Lying about car emissions means dirtier air, which is especially of concern in areas like Las Vegas that are already experiencing air quality problems,\" said Cynthia Giles, Assistant Administrator for Enforcement and Compliance Assurance at EPA. \"We will take aggressive action to ensure communities have clean air.\"    The maximum penalty for the felony violations contained in the indictments includes up to two years in prison and a fine of up to $250,000.    An indictment is merely an accusation, and a defendant is presumed innocent unless and until proven guilty in a court of law.    The case was investigated by the EPA, Criminal Investigation Division; and the Nevada Department of Motor Vehicles Compliance Enforcement Division. The case is being prosecuted by the U.S. Attorney’s Office for the District of Nevada and the Justice Department’s Environmental Crimes Section.</td>\n",
       "      <td>2010-01-08T00:00:00-05:00</td>\n",
       "      <td>No topic</td>\n",
       "      <td>Environment and Natural Resources Division</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18-898</td>\n",
       "      <td>$100 Million Settlement Will Speed Cleanup Work at Centredale Manor Superfund Site in North Providence, R.I.</td>\n",
       "      <td>The U.S. Department of Justice, the U.S. Environmental Protection Agency (EPA), and the Rhode Island Department of Environmental Management (RIDEM) announced today that two subsidiaries of Stanley Black &amp; Decker Inc.—Emhart Industries Inc. and Black &amp; Decker Inc.—have agreed to clean up dioxin contaminated sediment and soil at the Centredale Manor Restoration Project Superfund Site in North Providence and Johnston, Rhode Island.  “We are pleased to reach a resolution through collaborative work with the responsible parties, EPA, and other stakeholders,” said Acting Assistant Attorney General Jeffrey H. Wood for the Justice Department's Environment and Natural Resources Division . “Today’s settlement ends protracted litigation and allows for important work to get underway to restore a healthy environment for citizens living in and around the Centredale Manor Site and the Woonasquatucket River.” “This settlement demonstrates the tremendous progress we are achieving working with responsible parties, states, and our federal partners to expedite sites through the entire Superfund remediation process,” said EPA Acting Administrator Andrew Wheeler. “The Centredale Manor Site has been on the National Priorities List for 18 years; we are taking charge and ensuring the Agency makes good on its promise to clean it up for the betterment of the environment and those communities affected.” “Successfully concluding this settlement paves the way for EPA to make good on our commitment to aggressively pursue cleaning up the Centredale Manor Superfund Site,” said EPA New England Regional Administrator Alexandra Dunn. “We are excited to get to work on the cleanup at this site, and get it closer to the goal of being fully utilized by the North Providence and Johnston communities.” “We are pleased that the collective efforts of the State of Rhode Island, EPA, and DOJ in these negotiations have concluded in this major milestone toward the cleanup of the Centredale Manor Restoration Superfund site and are consistent with our long-standing efforts to make the polluter pay,” said RIDEM Director Janet Coit. “The settlement will speed up a remedy that protects public health and the river environment, and moves us closer to the day that we can reclaim recreational uses of this beautiful river resource.” The settlement, which includes cleanup work in the Woonasquatucket River (River) and bordering residential and commercial properties along the River, requires the companies to perform the remedy selected by EPA for the Site in 2012, which is estimated to cost approximately $100 million, and resolves longstanding litigation. The cleanup remedy includes excavation of contaminated sediment and floodplain soil from the Woonasquatucket River, including from adjacent residential properties. Once the cleanup remedy is completed, full access to the Woonasquatucket River should be restored for local citizens. The cleanup will be a step toward the State’s goal of a fishable and swimmable river. The work will also include upgrading caps over contaminated soil in the peninsula area of the Site that currently house two high-rise apartment buildings. The settlement also ensures that the long-term monitoring and maintenance of the site, as directed in the remedy, will be implemented to ensure that public health is protected.  Under the settlement, Emhart and Black &amp; Decker will reimburse EPA for approximately $42 million in past costs incurred at the Site. The companies will also reimburse EPA and the State of Rhode Island for future costs incurred by those agencies in overseeing the work required by the settlement. The settlement will also include payments on behalf of two federal agencies to resolve claims against those agencies. These payments, along with prior settlements related to the Site, will result in a 100 percent recovery for the United States of its past and future response costs related to the Site. Litigation related to the Site has been ongoing for nearly eight years. While the Federal District Court found Black &amp; Decker and Emhart to be liable for their hazardous waste and responsible to conduct the cleanup of the Site, it had also ruled that EPA needed to reconsider certain aspects of that cleanup. EPA appealed the decision requiring it to reconsider aspects of the cleanup. This settlement, once entered by the District Court, will resolve the litigation between the United States, Rhode Island, and Emhart and Black and Decker, allowing the cleanup of the Site to begin. The Site spans a one and a half mile stretch of the Woonasquatucket River and encompasses a nine-acre peninsula, two ponds and a significant forested wetland. From the 1940s to the early 1970s, Emhart’s predecessor operated a chemical manufacturing facility on the peninsula and used a raw material that was contaminated with 2,3,7,8-tetrachlorodibenzo-p-dioxin, a toxic form of dioxin. The Site property was also previously used by a barrel refurbisher. Elevated levels of dioxins and other contaminants have been detected in soil, groundwater, sediment, surface water and fish.  The Site was added to the National Priorities List (NPL) in 2000, and in December 2017, EPA included the Centredale Manor Restoration Project Superfund Site on a list of Superfund sites targeted for immediate and intense attention. Several short-term actions were previously performed at the Site to address immediate threats to the residents and minimize potential erosion and downstream transport of contaminated soil and sediment. This settlement is the latest agreement EPA has reached since the Site was listed on the NPL. Prior agreements addressed the performance and recovery of costs for the past environmental investigations and interim cleanup actions from Emhart, the barrel reconditioning company, the current owners of the peninsula portion of the Site, and other potentially responsible parties. The Consent Decree, lodged in the U.S. District Court of Rhode Island, will be posted in the Federal Register and available for public comment for a period of 30 days. The Consent Decree can be viewed on the Justice Department website: www.justice.gov/enrd/Consent_Decrees.html.  EPA information on the Centredale Manor Superfund Site: www.epa.gov/superfund/centredale.</td>\n",
       "      <td>2018-07-09T00:00:00-04:00</td>\n",
       "      <td>Environment</td>\n",
       "      <td>Environment and Natural Resources Division</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  \\\n",
       "0     None   \n",
       "1  12-919    \n",
       "2  11-1002   \n",
       "3   10-015   \n",
       "4   18-898   \n",
       "\n",
       "                                                                                                          title  \\\n",
       "0                                                                  Convicted Bomb Plotter Sentenced to 30 Years   \n",
       "1                              $1 Million in Restitution Payments Announced to Preserve North Carolina Wetlands   \n",
       "2                 $1 Million Settlement Reached for Natural Resource Damages at Superfund Site in Massachusetts   \n",
       "3                                          10 Las Vegas Men Indicted \\r\\nfor Falsifying Vehicle Emissions Tests   \n",
       "4  $100 Million Settlement Will Speed Cleanup Work at Centredale Manor Superfund Site in North Providence, R.I.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          contents  \\\n",
       "0  PORTLAND, Oregon. – Mohamed Osman Mohamud, 23, who was convicted in 2013 of attempting to use a weapon of mass destruction (explosives) in connection with a plot to detonate a vehicle bomb at an annual Christmas tree lighting ceremony in Portland, was sentenced today to serve 30 years in prison, followed by a lifetime term of supervised release. Mohamud, a naturalized U.S. citizen from Somalia and former resident of Corvallis, Oregon, was arrested on Nov. 26, 2010, after he attempted to detonate what he believed to be an explosives-laden van that was parked near the tree lighting ceremony in Portland.  The arrest was the culmination of a long-term undercover operation, during which Mohamud was monitored closely for months as his bomb plot developed.  The device was in fact inert, and the public was never in danger from the device. At sentencing, United States District Court Judge Garr M. King, who presided over Mohamed’s 14-day trial, said “the intended crime was horrific,” and that the defendant, even though he was presented with options by undercover FBI employees, “never once expressed a change of heart.”  King further noted that the Christmas tree ceremony was attended by up to 10,000 people, and that the defendant “wanted everyone to leave either dead or injured.”  King said his sentence was necessary in view of the seriousness of the crime and to serve as deterrence to others who might consider similar acts.     “With today’s sentencing, Mohamed Osman Mohamud is being held accountable for his attempted use of what he believed to be a massive bomb to attack innocent civilians attending a public Christmas tree lighting ceremony in Portland,” said John P. Carlin, Assistant Attorney General for National Security.  “The evidence clearly indicated that Mohamud was intent on killing as many people as possible with his attack.  Fortunately, law enforcement was able to identify him as a threat, insert themselves in the place of a terrorist that Mohamud was trying to contact, and thwart Mohamud’s efforts to conduct an attack on our soil.  This case highlights how the use of undercover operations against would-be terrorists allows us to engage and disrupt those who wish to commit horrific acts of violence against the innocent public.  The many agents, analysts, and prosecutors who have worked on this case deserve great credit for their roles in protecting Portland from the threat posed by this defendant and ensuring that he was brought to justice.” “This trial provided a rare glimpse into the techniques Al Qaeda employs to radicalize home-grown extremists,” said Amanda Marshall, U.S. Attorney for the District of Oregon.  “With the sentencing today, the court has held this defendant accountable.   I thank the dedicated professionals in the law enforcement and intelligence communities who were responsible for this successful outcome.  I look forward to our continued work with Muslim communities in Oregon who are committed to ensuring that all young people are safe from extremists who seek to radicalize others to engage in violence.”  According to the trial evidence, in February 2009, Mohamud began communicating via e-mail with Samir Khan, a now-deceased al Qaeda terrorist who published Jihad Recollections, an online magazine that advocated violent jihad, and who also published Inspire, the official magazine of al-Qaeda in the Arabian Peninsula.  Between February and August 2009, Mohamed exchanged approximately 150 emails with Khan.  Mohamud wrote several articles for Jihad Recollections that were published under assumed names. In August 2009, Mohamud was in email contact with Amro Al-Ali, a Saudi national who was in Yemen at the time and is today in custody in Saudi Arabia for terrorism offenses.  Al-Ali sent Mohamud detailed e-mails designed to facilitate Mohamud’s travel to Yemen to train for violent jihad.  In December 2009, while Al-Ali was in the northwest frontier province of Pakistan, Mohamud and Al-Ali discussed the possibility of Mohamud traveling to Pakistan to join Al-Ali in terrorist activities. Mohamud responded to Al-Ali in an e-mail: “yes, that would be wonderful, just tell me what I need to do.”  Al-Ali referred Mohamud to a second associate overseas and provided Mohamud with a name and email address to facilitate the process. In the following months, Mohamud made several unsuccessful attempts to contact Al-Ali’s associate.  Ultimately, an FBI undercover operative contacted Mohamud via email under the guise of being an associate of Al-Ali’s.  Mohamud and the FBI undercover operative agreed to meet in Portland in July 2010.  At the meeting, Mohamud told the FBI undercover operative he had written articles that were published in Jihad Recollections.  Mohamud also said that he wanted to become “operational.”  Asked what he meant by “operational,” Mohamud said he wanted to put an explosion together, but needed help. According to evidence presented at trial, at a meeting in August 2010, Mohamud told undercover FBI operatives he had been thinking of committing violent jihad since the age of 15.  Mohamud then told the undercover FBI operatives that he had identified a potential target for a bomb: the annual Christmas tree lighting ceremony in Portland’s Pioneer Courthouse Square on Nov. 26, 2010.  The undercover FBI operatives cautioned Mohamud several times about the seriousness of this plan, noting there would be many people at the event, including children, and emphasized that Mohamud could abandon his attack plans at any time with no shame.  Mohamud indicated the deaths would be justified and that he would not mind carrying out a suicide attack on the crowd. According to evidence presented at trial, in the ensuing months Mohamud continued to express his interest in carrying out the attack and worked on logistics.  On Nov. 4, 2010, Mohamud and the undercover FBI operatives traveled to a remote location in Lincoln County, Oregon, where they detonated a bomb concealed in a backpack as a trial run for the upcoming attack.  During the drive back to Corvallis, Mohamud was asked if was capable looking at all the bodies of those who would be killed during the explosion.  In response, Mohamud noted, “I want whoever is attending that event to be, to leave either dead or injured.”  Mohamud later recorded a video of himself, with the assistance of the undercover FBI operatives, in which he read a statement that offered his rationale for his bomb attack.  On Nov. 18, 2010, undercover FBI operatives picked up Mohamud to travel to Portland to finalize the details of the attack.  On Nov. 26, 2010, just hours before the planned attack, Mohamud examined the 1,800 pound bomb in the van and remarked that it was “beautiful.”  Later that day, Mohamud was arrested after he attempted to remotely detonate the inert vehicle bomb rked near the Christmas tree lighting ceremony This case was investigated by the FBI, with assistance from the Oregon State Police, the Corvallis Police Department, the Lincoln County Sheriff’s Office and the Portland Police Bureau.  The prosecution was handled by Assistant U.S. Attorneys Ethan D. Knight and Pamala Holsinger from the U.S. Attorney’s Office for the District of Oregon.  Trial Attorney Jolie F. Zimmerman, from the Counterterrorism Section of the Justice Department’s National Security Division, assisted. # # # 14-1077   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       WASHINGTON – North Carolina’s Waccamaw River watershed will benefit from a $1 million restitution order from a federal court, funding environmental projects to acquire and preserve wetlands in an area damaged by illegal releases of wastewater from a corporate hog farm, announced Ignacia S. Moreno, Assistant Attorney General of the Justice Department’s Environment and Natural Resources Division; U.S. Attorney for the Eastern District of North Carolina Thomas G. Walker; Director Greg McLeod from the North Carolina State Bureau of Investigation; and Camilla M. Herlevich, Executive Director of the North Carolina Coastal Land Trust.   Freedman Farms Inc. was sentenced in February 2012 to five years of probation and ordered to pay $1.5 million in fines, restitution and community service payments for violating the Clean Water Act when it discharged hog waste into a stream that leads to the Waccamaw River.  William B. Freedman, president of Freedman Farms, was sentenced to six months in prison to be followed by six months of home confinement.  Freedman Farms also is required to implement a comprehensive environmental compliance program and institute an annual training program.   In an order issued on April 19, 2012, the court ordered that the defendants would be responsible for restitution of $1 million in the form of five annual payments starting in January 2013, which the court will direct to the North Carolina Coastal Land Trust (NCCLT).  The NCCLT plans to use the money to acquire and conserve land along streams in the Waccamaw watershed.  The court also directed a $75,000 community service payment to the Southern Environmental Enforcement Network, an organization dedicated to environmental law enforcement training and information sharing in the region.    “The resolution of the case against Freedman Farms demonstrates the commitment of the Department of Justice to enforcing the Clean Water Act to ensure the protection of human health and the environment,” said Assistant Attorney General Moreno.  “The court-ordered restitution in this case will conserve wetlands for the benefit of the people of North Carolina.  By enforcing the nation’s environmental laws, we will continue to ensure that concentrated animal feeding operations (CAFOs) operate without threatening our drinking water, the health of our communities and the environment.”   “This office is committed to doing our part to hold accountable those who commit crimes against our environment, which can cause serious health problems to residents and damage the environment that makes North Carolina such a beautiful place to live and visit,” said U.S. Attorney Walker.   “This case shows what we can accomplish when our SBI agents work closely with their local, state and federal partners to investigate environmental crimes and hold the polluters accountable,” said Director McLeod.  “We’ll continue our efforts to fight illegal pollution that damages our water and puts the public’s health at risk.”    “The Waccamaw is unique and wild,” said Director Herlevich of the North Carolina Coastal Land Trust. “Its watershed includes some of the most extensive cypress gum swamps in the state, and its headwaters at Lake Waccamaw contain fish that are found nowhere else on Earth.  We appreciate the trust of the court and the U. S. Attorney, and we look forward to using these funds for conservation projects in a river system that is one of our top conservation priorities.”   According to evidence presented in court, in December 2007 Freedman Farms discharged hog waste into Browder’s Branch, a tributary to the Waccamaw River that flows through the White Marsh, a large wetlands complex.  Freedman Farms, located in Columbus County, N.C., is in the business of raising hogs for market, and this particular farm had some 4,800 hogs.  The hog waste was supposed to be directed to two lagoons for treatment and disposal.  Instead, hog waste was discharged from Freedman Farms directly into Browder’s Branch.    The Clean Water Act is a federal law that makes it illegal to knowingly or negligently discharge a pollutant into a water of the United States.    The Freedman case was investigated by the U.S. Environmental Protection Agency (EPA) Criminal Investigation Division, the U.S. Army Corps of Engineers and the North Carolina State Bureau of Investigation, with assistance from the EPA Science and Ecosystem Support Division.  The case was prosecuted by Assistant U.S. Attorney J. Gaston B. Williams of the Eastern District of North Carolina and Trial Attorney Mary Dee Carraway of the Environmental Crimes Section of the Justice Department’s Environment and Natural Resources Division.   The North Carolina Coastal Land Trust is celebrating its 20th anniversary of saving special lands in eastern North Carolina. The organization has protected nearly 50,000 acres of lands with scenic, recreational, historic and ecological values. North Carolina Coastal Land Trust has saved streams and wetlands that provide clean water, forests that are havens for wildlife, working farms that provide local food and nature parks that everyone can enjoy.  More information about the Coastal Land Trust is available at www.coastallandtrust.org.   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        BOSTON– A $1-million settlement has been reached for natural resource damages (NRD) at the Blackburn & Union Privileges Superfund Site in Walpole, Mass., the Departments of Justice and Interior (DOI), and the Office of the Massachusetts Attorney General announced today.                The Blackburn & Union Privileges Superfund Site includes 22 acres of contaminated land and water in Walpole. The contamination resulted from the operations of various industrial facilities dating back to the 19th century that exposed the site to asbestos, arsenic, lead and other hazardous substances.                The private parties involved in the settlement include two former owners and operators of the site, W.R. Grace & Co.– Conn. and Tyco Healthcare Group LP, as well as the current owners, BIM Investment Corp. and Shaffer Realty Nominee Trust.               From about 1915 to 1936, a predecessor of W.R. Grace manufactured asbestos brake linings and clutch linings on a large portion of the property. From 1946 to about 1983, a predecessor of Tyco Healthcare operated a cotton fabric manufacturing business, which used caustic solutions, on a portion of the property.               In a 2010 settlement with U.S. Environmental Protection Agency (EPA), the four private parties agreed to perform a remedial action to clean up the site at an estimated cost of $13 million. The consent decree lodged today resolves both state and federal NRD liability claims; it requires the parties to pay $1,094,169.56 to the state and federal natural resource trustees, the Massachusetts Executive Office of Energy and Environmental Affairs (EEA) and DOI, for injuries to ecological resources including groundwater and wetlands, which provide habitat for waterfowl and wading birds, including black ducks and great blue herons.  The trustees will use the settlement funds for natural resource restoration projects in the area.               “This settlement demonstrates our commitment to recovering damages from the parties responsible for injury to natural resources, in partnership with state trustees,” said Bruce Gelber, Acting Deputy Assistant Attorney General of the Justice Department’s Environment and Natural Resources Division.               “The citizens of Walpole have had to live with the environmental impact of this contamination for many years,” Attorney General Martha Coakley said. “We are pleased that today’s agreement will not only require the responsible parties to reimburse taxpayer dollars, but will also provide funding to begin restoring or replacing the wetland and other natural resources.”                 The consent decree was lodged in the U.S. District Court for Massachusetts.     A portion of the funds, $300,000, will be distributed to the EEA-sponsored groundwater restoration projects; $575,000 will be used for ecological restoration projects jointly sponsored by EEA and the U.S. Fish and Wildlife Service (FWS).               In addition, $125,000 will go for projects jointly sponsored by EEA and FWS that achieve both ecological and groundwater restoration; $57,491.34 will be allocated for reimbursement for the FWS’s assessment costs; and $36,678.22 will be distributed as reimbursement for the commonwealth’s assessment costs.       “This settlement provides the means for a range of projects designed to compensate the public for decades of groundwater and other ecological damage at this site.  I encourage local citizens and organizations to become engaged in the public process that will take place as we solicit, take comment on, and choose these projects in the months ahead,” said Energy and Environmental Affairs Secretary Richard K. Sullivan Jr., who serves as the Commonwealth’s Natural Resources Damages trustee.       “This settlement will help restore habitat for fish and wildlife in the Neponset River watershed,” said Tom Chapman of the FWS New England Field Office. “We look forward to working with the commonwealth and local stakeholders to implement restoration.”               “More than 100 years-worth of industrial activities at this site caused major environmental contamination to the Neponset River, nearby wetlands and to groundwater below the site,” said Commissioner Kenneth Kimmell of the Massachusetts Department of Environmental Protection (MassDEP), which will staff the Trustee Council for the Commonwealth. “We will ensure that the community and the public will be active participants in the process to use these NRD funds to restore the injured natural resources.”                Under the federal Comprehensive Environmental Response, Compensation and Liability Act, EEA and DOI, acting through the FWS, are the designated state and federal natural resource Trustees for the site. The site has been listed on the EPA’s National Priorities List since 1994.        The consent decree is subject to a public comment period and court approval. A copy of the consent decree and instructions about how to submit comments is available on www.usdoj.gov/enrd/Consent_Decrees.html  .               After the consent decree is approved, EEA and FWS will develop proposed restoration plans to use the settlement funds for restoration projects. The proposed restoration plans will also be made available to the public for review and comment.                Assistant Attorney General Matthew Brock of Massachusetts Attorney General Coakley's Environmental Protection Division handled this matter.  Attorney Jennifer Davis of MassDEP, Attorney Anna Blumkin of EEA and MassDEP’s NRD Coordinator Karen Pelto also worked on this settlement.   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           WASHINGTON—A federal grand jury in Las Vegas today returned indictments against 10 Nevada-certified emissions testers for falsifying vehicle emissions test reports, the Justice Department announced.   Each defendant faces one felony Clean Air Act count for falsifying reports between November 2007 and May 2009. The number of falsifications varied by defendant, with some defendants having falsified approximately 250 records, while others falsified more than double that figure. One defendant is alleged to have falsified over 700 reports.   The individuals indicted include:     Escudero resides in Pahrump, Nev. All other individuals are from Clark County, Nev.    The 10 defendants are alleged to have engaged in a practice known as \"clean scanning\" vehicles. The scheme involved entering the Vehicle Identification Number (VIN) for a vehicle that would not pass the emissions test into the computerized system, then connecting a different vehicle the testers knew would pass the test. These falsifications were allegedly performed for anywhere from $10 to $100 over and above the usual emissions testing fee.    The U.S. Environmental Protection Agency (EPA), under the Clean Air Act, requires the state of Nevada to conduct vehicle emissions testing in certain areas because the areas exceed national standards for carbon monoxide and ozone. Las Vegas is currently required to perform emissions testing.    To obtain a registration renewal, vehicle owners bring the vehicles to a licensed inspection station for testing. The emissions inspector logs into a computer to activate the system by using a unique password issued to the emissions inspector. The emissions inspector manually inputs the vehicle’s VIN to identify the tested vehicle, then connects the vehicle for model year 1996 and later to an onboard diagnostics port connected to an analyzer. The analyzer downloads data from the vehicle’s computer, analyzes the data and provides a \"pass\" or \"fail\" result. The pass or fail result and vehicle identification data are reported on the Vehicle Inspection Report. It is a crime to knowingly alter or conceal any record or other document required to be maintained by the Clean Air Act.     \"Falsifications of vehicle emissions testing, such as those alleged in the indictments unsealed today, are serious matters and we intend to use all of our enforcement tools to stop this harmful practice. These actions undermine a system that is designed to reduce air pollutants including smog and provide better air quality for the citizens of Nevada,\" said Ignacia S. Moreno, Assistant Attorney General for the Justice Department’s Environment and Natural Resources Division.    \"The residents of Nevada deserve to know that the vast majority of licensed vehicle emission inspectors are not corrupt and are not circumventing emission testing procedures,\" said U.S. Attorney Bogden. \"These indictments should serve as a clear warning to offenders that the Department of Justice will prosecute you if you make fraudulent statements and reports concerning compliance with the federal Clean Air Act.\"    \"Lying about car emissions means dirtier air, which is especially of concern in areas like Las Vegas that are already experiencing air quality problems,\" said Cynthia Giles, Assistant Administrator for Enforcement and Compliance Assurance at EPA. \"We will take aggressive action to ensure communities have clean air.\"    The maximum penalty for the felony violations contained in the indictments includes up to two years in prison and a fine of up to $250,000.    An indictment is merely an accusation, and a defendant is presumed innocent unless and until proven guilty in a court of law.    The case was investigated by the EPA, Criminal Investigation Division; and the Nevada Department of Motor Vehicles Compliance Enforcement Division. The case is being prosecuted by the U.S. Attorney’s Office for the District of Nevada and the Justice Department’s Environmental Crimes Section.   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   The U.S. Department of Justice, the U.S. Environmental Protection Agency (EPA), and the Rhode Island Department of Environmental Management (RIDEM) announced today that two subsidiaries of Stanley Black & Decker Inc.—Emhart Industries Inc. and Black & Decker Inc.—have agreed to clean up dioxin contaminated sediment and soil at the Centredale Manor Restoration Project Superfund Site in North Providence and Johnston, Rhode Island.  “We are pleased to reach a resolution through collaborative work with the responsible parties, EPA, and other stakeholders,” said Acting Assistant Attorney General Jeffrey H. Wood for the Justice Department's Environment and Natural Resources Division . “Today’s settlement ends protracted litigation and allows for important work to get underway to restore a healthy environment for citizens living in and around the Centredale Manor Site and the Woonasquatucket River.” “This settlement demonstrates the tremendous progress we are achieving working with responsible parties, states, and our federal partners to expedite sites through the entire Superfund remediation process,” said EPA Acting Administrator Andrew Wheeler. “The Centredale Manor Site has been on the National Priorities List for 18 years; we are taking charge and ensuring the Agency makes good on its promise to clean it up for the betterment of the environment and those communities affected.” “Successfully concluding this settlement paves the way for EPA to make good on our commitment to aggressively pursue cleaning up the Centredale Manor Superfund Site,” said EPA New England Regional Administrator Alexandra Dunn. “We are excited to get to work on the cleanup at this site, and get it closer to the goal of being fully utilized by the North Providence and Johnston communities.” “We are pleased that the collective efforts of the State of Rhode Island, EPA, and DOJ in these negotiations have concluded in this major milestone toward the cleanup of the Centredale Manor Restoration Superfund site and are consistent with our long-standing efforts to make the polluter pay,” said RIDEM Director Janet Coit. “The settlement will speed up a remedy that protects public health and the river environment, and moves us closer to the day that we can reclaim recreational uses of this beautiful river resource.” The settlement, which includes cleanup work in the Woonasquatucket River (River) and bordering residential and commercial properties along the River, requires the companies to perform the remedy selected by EPA for the Site in 2012, which is estimated to cost approximately $100 million, and resolves longstanding litigation. The cleanup remedy includes excavation of contaminated sediment and floodplain soil from the Woonasquatucket River, including from adjacent residential properties. Once the cleanup remedy is completed, full access to the Woonasquatucket River should be restored for local citizens. The cleanup will be a step toward the State’s goal of a fishable and swimmable river. The work will also include upgrading caps over contaminated soil in the peninsula area of the Site that currently house two high-rise apartment buildings. The settlement also ensures that the long-term monitoring and maintenance of the site, as directed in the remedy, will be implemented to ensure that public health is protected.  Under the settlement, Emhart and Black & Decker will reimburse EPA for approximately $42 million in past costs incurred at the Site. The companies will also reimburse EPA and the State of Rhode Island for future costs incurred by those agencies in overseeing the work required by the settlement. The settlement will also include payments on behalf of two federal agencies to resolve claims against those agencies. These payments, along with prior settlements related to the Site, will result in a 100 percent recovery for the United States of its past and future response costs related to the Site. Litigation related to the Site has been ongoing for nearly eight years. While the Federal District Court found Black & Decker and Emhart to be liable for their hazardous waste and responsible to conduct the cleanup of the Site, it had also ruled that EPA needed to reconsider certain aspects of that cleanup. EPA appealed the decision requiring it to reconsider aspects of the cleanup. This settlement, once entered by the District Court, will resolve the litigation between the United States, Rhode Island, and Emhart and Black and Decker, allowing the cleanup of the Site to begin. The Site spans a one and a half mile stretch of the Woonasquatucket River and encompasses a nine-acre peninsula, two ponds and a significant forested wetland. From the 1940s to the early 1970s, Emhart’s predecessor operated a chemical manufacturing facility on the peninsula and used a raw material that was contaminated with 2,3,7,8-tetrachlorodibenzo-p-dioxin, a toxic form of dioxin. The Site property was also previously used by a barrel refurbisher. Elevated levels of dioxins and other contaminants have been detected in soil, groundwater, sediment, surface water and fish.  The Site was added to the National Priorities List (NPL) in 2000, and in December 2017, EPA included the Centredale Manor Restoration Project Superfund Site on a list of Superfund sites targeted for immediate and intense attention. Several short-term actions were previously performed at the Site to address immediate threats to the residents and minimize potential erosion and downstream transport of contaminated soil and sediment. This settlement is the latest agreement EPA has reached since the Site was listed on the NPL. Prior agreements addressed the performance and recovery of costs for the past environmental investigations and interim cleanup actions from Emhart, the barrel reconditioning company, the current owners of the peninsula portion of the Site, and other potentially responsible parties. The Consent Decree, lodged in the U.S. District Court of Rhode Island, will be posted in the Federal Register and available for public comment for a period of 30 days. The Consent Decree can be viewed on the Justice Department website: www.justice.gov/enrd/Consent_Decrees.html.  EPA information on the Centredale Manor Superfund Site: www.epa.gov/superfund/centredale.   \n",
       "\n",
       "                        date topics_clean  \\\n",
       "0  2014-10-01T00:00:00-04:00     No topic   \n",
       "1  2012-07-25T00:00:00-04:00     No topic   \n",
       "2  2011-08-03T00:00:00-04:00     No topic   \n",
       "3  2010-01-08T00:00:00-05:00     No topic   \n",
       "4  2018-07-09T00:00:00-04:00  Environment   \n",
       "\n",
       "                             components_clean  \n",
       "0            National Security Division (NSD)  \n",
       "1  Environment and Natural Resources Division  \n",
       "2  Environment and Natural Resources Division  \n",
       "3  Environment and Natural Resources Division  \n",
       "4  Environment and Natural Resources Division  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## first, unzip the file pset3_inputdata.zip \n",
    "## then, run this code to load the unzipped json file and convert to a dataframe\n",
    "## (may need to change the pathname depending on where you store stuff)\n",
    "## and convert some of the attributes from lists to values\n",
    "doj = pd.read_json(\"combined.json\", lines = True)\n",
    "\n",
    "## due to json, topics are in a list so remove them and concatenate with ;\n",
    "doj['topics_clean'] = [\"; \".join(topic) \n",
    "                      if len(topic) > 0 else \"No topic\" \n",
    "                      for topic in doj.topics]\n",
    "\n",
    "## similarly with components\n",
    "doj['components_clean'] = [\"; \".join(comp) \n",
    "                           if len(comp) > 0 else \"No component\" \n",
    "                           for comp in doj.components]\n",
    "\n",
    "## drop older columns from data\n",
    "doj = doj[['id', 'title', 'contents', 'date', 'topics_clean', \n",
    "           'components_clean']].copy()\n",
    "\n",
    "doj.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tagging and sentiment scoring (17 points)\n",
    "\n",
    "Focus on the following press release: `id` == \"17-1204\" about this pharmaceutical kickback prosecution: https://www.forbes.com/sites/michelatindera/2017/11/16/fentanyl-billionaire-john-kapoor-to-plead-not-guilty-in-opioid-kickback-case/?sh=21b8574d6c6c \n",
    "\n",
    "The `contents` column is the one we're treating as a document. You may need to to convert it from a pandas series to a single string.\n",
    "\n",
    "We'll call the raw string of this press release `pharma`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to get the press release with ID \"17-1204\"\n",
    "pharma_release = doj[doj['id'] == \"17-1204\"]\n",
    "\n",
    "# Extract the contents of the press release\n",
    "pharma_contents = pharma_release['contents'].iloc[0]\n",
    "\n",
    "# Convert the contents to a single string\n",
    "pharma = str(pharma_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 part of speech tagging (3 points)\n",
    "\n",
    "A. Preprocess the `pharma` press release to remove all punctuation / digits (you can use `.isalpha()` to subset)\n",
    "\n",
    "B. With the preprocessed press release from part A, use the part of speech tagger within nltk to tag all the words in that one press release with their part of speech. \n",
    "\n",
    "C. Using the output from B, extract the adjectives and sort those adjectives from most occurrences to fewest occurrences. Print a dataframe with the 5 most frequent adjectives and their counts in the `pharma` release. See here for a list of the names of adjectives within nltk: https://pythonprogramming.net/natural-language-toolkit-nltk-part-speech-tagging/\n",
    "\n",
    "**Resources**:\n",
    "\n",
    "- Documentation for `.isalpha()`: https://www.w3schools.com/python/ref_string_isalpha.asp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part A\n",
    "preprocessed_pharma1 = re.sub(r'\\W+', ' ', pharma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part B\n",
    "tokens = word_tokenize(preprocessed_pharma)\n",
    "tagged_words = nltk.pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Adjective  Count\n",
      "10      former      8\n",
      "27      opioid      5\n",
      "0   nationwide      4\n",
      "31   addictive      3\n",
      "9        other      3\n"
     ]
    }
   ],
   "source": [
    "# Part C\n",
    "from collections import Counter\n",
    "\n",
    "# Extract adjectives from tagged words\n",
    "adjectives = [word for word, tag in tagged_words if tag.startswith('JJ')]\n",
    "\n",
    "# Count occurrences of adjectives\n",
    "adj_counts = Counter(adjectives)\n",
    "\n",
    "# Convert the Counter to a DataFrame\n",
    "adj_df = pd.DataFrame(adj_counts.items(), columns=['Adjective', 'Count'])\n",
    "\n",
    "# Sort the DataFrame by count in descending order\n",
    "adj_df = adj_df.sort_values(by='Count', ascending=False)\n",
    "print(adj_df.head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 named entity recognition (4 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A. Using the original `pharma` press release (so the one before stripping punctuation/digits), use spaCy to extract all named entities from the press release.\n",
    "\n",
    "B. Print the unique named entities with the tag: `LAW`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part A\n",
    "# Load the English language model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Process the original pharma press release\n",
    "doc = nlp(pharma)\n",
    "\n",
    "# Extract named entities\n",
    "named_entities = [(entity.text, entity.label_) for entity in doc.ents]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Named Entities with Tag 'LAW': {'the Controlled Substances Act', 'RICO'}\n"
     ]
    }
   ],
   "source": [
    "# Part B\n",
    "print(\"Unique Named Entities with Tag 'LAW':\", set(entity.text for entity in doc.ents if entity.label_ == 'LAW'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C. Use Google to summarize in one sentence what the `RICO` named entity means and why this might apply to a pharmaceutical kickbacks case (and not just a mafia case...) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main aim of the Racketeer Influenced and Corrupt Organizations Act (RICO) was to combat organized crime like the Mafia. But over time, some have tried to stretch its scope to create a federal law against deceptive advertising by pharmaceutical companies, seeking hefty compensation and legal costs. The Ninth Circuit's recent decision has further supported these attempts by allowing RICO claims against pharmaceutical firms for their alleged mislabeling and promotional practices concerning prescription drugs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D. You want to extract the possible sentence lengths the CEO is facing; pull out the named entities with (1) the label `DATE` and (2) that contain the word year or years (hint: you may want to use the `re` module for that second part). Print these named entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Possible sentence lengths the CEO is facing:\n",
      "last year\n",
      "20 years\n",
      "three years\n",
      "five years\n",
      "three years\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Extract named entities with the label \"DATE\"\n",
    "date_entities = [entity.text for entity in doc.ents if entity.label_ == 'DATE']\n",
    "\n",
    "# Filter date entities containing the word \"year\" or \"years\" using regex\n",
    "sentence_lengths = [date for date in date_entities if re.search(r'\\b(?:year|years)\\b', date, flags=re.IGNORECASE)]\n",
    "\n",
    "# Print the extracted possible sentence lengths the CEO is facing\n",
    "print(\"Possible sentence lengths the CEO is facing:\")\n",
    "for length in sentence_lengths:\n",
    "    print(length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E. Pull and print the original parts of the press releases where those year lengths are mentioned (e.g., the sentences or rough region of the press release). Describe in your own words (1 sentence) what length of sentence (prison) and probation (supervised release) the CEO may be facing if convicted after this indictment (if there are multiple lengths mentioned describe the maximum). \n",
    "\n",
    "**Hint**: you may want to use re.search or re.findall \n",
    "\n",
    "- For part E, you can use `re.search` and `re.findall`, or anything that works 😳."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Parts of the Press Release where Year Lengths are Mentioned:\n",
      " \"More than 20,000 Americans died of synthetic opioid overdoses last year, and millions are addicted to opioids.\n",
      "The charges of conspiracy to commit RICO and conspiracy to commit mail and wire fraud each provide for a sentence of no greater than 20 years in prison, three years of supervised release and a fine of $250,000, or twice the amount of pecuniary gain or loss.\n",
      "  The charges of conspiracy to violate the Anti-Kickback Law provide for a sentence of no greater than five years in prison, three years of supervised release and a $25,000 fine.\n",
      "\n",
      "Description:\n",
      "The CEO may be facing a maximum sentence of 20 years in prison.\n",
      "If convicted after this indictment, the CEO may face a maximum supervised release (probation) of 20 years.\n"
     ]
    }
   ],
   "source": [
    "# Define a regular expression pattern to match sentences containing the identified year lengths\n",
    "pattern = r'(?:^|(?<=[.!?]))[^.!?]*\\b(?:year|years)\\b[^.!?]*(?:[.!?]|$)'\n",
    "\n",
    "# Find all sentences matching the pattern in the pharma press release\n",
    "matching_sentences = re.findall(pattern, pharma)\n",
    "\n",
    "# Print the original parts of the press releases where year lengths are mentioned\n",
    "print(\"Original Parts of the Press Release where Year Lengths are Mentioned:\")\n",
    "for sentence in matching_sentences:\n",
    "    print(sentence)\n",
    "\n",
    "# Describe the potential sentence and probation lengths the CEO may face\n",
    "if matching_sentences:\n",
    "    # Extract the maximum year length mentioned\n",
    "    max_year_length = max([int(re.search(r'\\b\\d+\\b', date).group()) for date in sentence_lengths if re.search(r'\\b\\d+\\b', date)])\n",
    "\n",
    "    # Describe the maximum sentence and probation lengths\n",
    "    sentence_description = f\"The CEO may be facing a maximum sentence of {max_year_length} years in prison.\"\n",
    "    probation_description = f\"If convicted after this indictment, the CEO may face a maximum supervised release (probation) of {max_year_length} years.\"\n",
    "\n",
    "    print(\"\\nDescription:\")\n",
    "    print(sentence_description)\n",
    "    print(probation_description)\n",
    "else:\n",
    "    print(\"\\nNo year lengths mentioned in the press release.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 sentiment analysis  (10 points)\n",
    "\n",
    "A. Subset the press releases to those labeled with one of three topics via `topics_clean`: Civil Rights, Hate Crimes, and Project Safe Childhood. We'll call this `doj_subset` going forward and it should have 717 rows.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in doj_subset: 717\n"
     ]
    }
   ],
   "source": [
    "# Define the list of topics to filter\n",
    "topics_to_filter = ['Civil Rights', 'Hate Crimes', 'Project Safe Childhood']\n",
    "\n",
    "# Filter press releases based on the specified topics\n",
    "doj_subset = doj[doj['topics_clean'].isin(topics_to_filter)]\n",
    "\n",
    "# Check the number of rows in the subset\n",
    "print(\"Number of rows in doj_subset:\", len(doj_subset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B. Write a function that takes one press release string as an input and:\n",
    "\n",
    "- Removes named entities from each press release string (**Hint**: you may want to use `re.sub` with an or condition)\n",
    "- Scores the sentiment of the entire press release using the `SentimentIntensityAnalyzer` and `polarity_scores`\n",
    "- Returns the length-four (negative, positive, neutral, compound) sentiment dictionary (any order is fine)\n",
    "\n",
    "Apply that function to each of the press releases in `doj_subset`. \n",
    "\n",
    "**Hints**: \n",
    "\n",
    "- A function + list comprehension to execute will takes about 30 seconds on a respectable local machine and about 2 mins on jhub; if it's taking a very long time, you may want to check your code for inefficiencies. If you can't fix those, for partial credit on this part/full credit on remainder, you can take a small random sample of the 717\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Initialize the SentimentIntensityAnalyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "def process_press_release(press_release):\n",
    "    # Remove named entities using regular expressions\n",
    "    processed_release = re.sub(r'\\b[A-Z][a-z]*\\b', '', press_release)\n",
    "    \n",
    "    # Score the sentiment of the processed string\n",
    "    sentiment_scores = sia.polarity_scores(processed_release)\n",
    "    \n",
    "    # Return the sentiment dictionary\n",
    "    return sentiment_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'neg': 0.207, 'neu': 0.739, 'pos': 0.054, 'compound': -0.992},\n",
       " {'neg': 0.125, 'neu': 0.799, 'pos': 0.075, 'compound': -0.8843},\n",
       " {'neg': 0.094, 'neu': 0.827, 'pos': 0.078, 'compound': -0.738},\n",
       " {'neg': 0.13, 'neu': 0.782, 'pos': 0.088, 'compound': -0.8981},\n",
       " {'neg': 0.187, 'neu': 0.767, 'pos': 0.046, 'compound': -0.9864},\n",
       " {'neg': 0.148, 'neu': 0.79, 'pos': 0.062, 'compound': -0.9833},\n",
       " {'neg': 0.156, 'neu': 0.763, 'pos': 0.081, 'compound': -0.9541},\n",
       " {'neg': 0.089, 'neu': 0.843, 'pos': 0.068, 'compound': -0.7128},\n",
       " {'neg': 0.106, 'neu': 0.83, 'pos': 0.064, 'compound': -0.9022},\n",
       " {'neg': 0.164, 'neu': 0.774, 'pos': 0.062, 'compound': -0.9733},\n",
       " {'neg': 0.227, 'neu': 0.735, 'pos': 0.038, 'compound': -0.9973},\n",
       " {'neg': 0.099, 'neu': 0.833, 'pos': 0.067, 'compound': -0.8417},\n",
       " {'neg': 0.089, 'neu': 0.839, 'pos': 0.071, 'compound': -0.617},\n",
       " {'neg': 0.341, 'neu': 0.628, 'pos': 0.031, 'compound': -0.9957},\n",
       " {'neg': 0.184, 'neu': 0.756, 'pos': 0.06, 'compound': -0.9907},\n",
       " {'neg': 0.129, 'neu': 0.807, 'pos': 0.064, 'compound': -0.9735},\n",
       " {'neg': 0.165, 'neu': 0.739, 'pos': 0.095, 'compound': -0.9908},\n",
       " {'neg': 0.244, 'neu': 0.7, 'pos': 0.056, 'compound': -0.9982},\n",
       " {'neg': 0.156, 'neu': 0.75, 'pos': 0.094, 'compound': -0.9786},\n",
       " {'neg': 0.138, 'neu': 0.784, 'pos': 0.078, 'compound': -0.9745},\n",
       " {'neg': 0.215, 'neu': 0.701, 'pos': 0.084, 'compound': -0.9952},\n",
       " {'neg': 0.163, 'neu': 0.82, 'pos': 0.017, 'compound': -0.9884},\n",
       " {'neg': 0.294, 'neu': 0.674, 'pos': 0.032, 'compound': -0.9985},\n",
       " {'neg': 0.212, 'neu': 0.742, 'pos': 0.046, 'compound': -0.995},\n",
       " {'neg': 0.188, 'neu': 0.733, 'pos': 0.079, 'compound': -0.982},\n",
       " {'neg': 0.15, 'neu': 0.787, 'pos': 0.063, 'compound': -0.9665},\n",
       " {'neg': 0.155, 'neu': 0.783, 'pos': 0.062, 'compound': -0.9846},\n",
       " {'neg': 0.243, 'neu': 0.727, 'pos': 0.03, 'compound': -0.9939},\n",
       " {'neg': 0.194, 'neu': 0.757, 'pos': 0.05, 'compound': -0.9716},\n",
       " {'neg': 0.212, 'neu': 0.735, 'pos': 0.053, 'compound': -0.9981},\n",
       " {'neg': 0.142, 'neu': 0.808, 'pos': 0.05, 'compound': -0.9607},\n",
       " {'neg': 0.111, 'neu': 0.758, 'pos': 0.131, 'compound': 0.4754},\n",
       " {'neg': 0.079, 'neu': 0.842, 'pos': 0.079, 'compound': -0.0094},\n",
       " {'neg': 0.107, 'neu': 0.762, 'pos': 0.131, 'compound': 0.4767},\n",
       " {'neg': 0.303, 'neu': 0.669, 'pos': 0.028, 'compound': -0.9977},\n",
       " {'neg': 0.073, 'neu': 0.788, 'pos': 0.139, 'compound': 0.9967},\n",
       " {'neg': 0.143, 'neu': 0.742, 'pos': 0.115, 'compound': -0.9648},\n",
       " {'neg': 0.235, 'neu': 0.662, 'pos': 0.103, 'compound': -0.9866},\n",
       " {'neg': 0.164, 'neu': 0.722, 'pos': 0.114, 'compound': -0.9942},\n",
       " {'neg': 0.285, 'neu': 0.624, 'pos': 0.091, 'compound': -0.9169},\n",
       " {'neg': 0.034, 'neu': 0.847, 'pos': 0.119, 'compound': 0.9255},\n",
       " {'neg': 0.179, 'neu': 0.769, 'pos': 0.052, 'compound': -0.975},\n",
       " {'neg': 0.125, 'neu': 0.824, 'pos': 0.051, 'compound': -0.9559},\n",
       " {'neg': 0.12, 'neu': 0.83, 'pos': 0.05, 'compound': -0.9813},\n",
       " {'neg': 0.091, 'neu': 0.834, 'pos': 0.075, 'compound': 0.0129},\n",
       " {'neg': 0.064, 'neu': 0.877, 'pos': 0.059, 'compound': -0.4215},\n",
       " {'neg': 0.198, 'neu': 0.754, 'pos': 0.047, 'compound': -0.9895},\n",
       " {'neg': 0.138, 'neu': 0.807, 'pos': 0.055, 'compound': -0.9501},\n",
       " {'neg': 0.126, 'neu': 0.8, 'pos': 0.074, 'compound': -0.9496},\n",
       " {'neg': 0.107, 'neu': 0.812, 'pos': 0.081, 'compound': -0.8595},\n",
       " {'neg': 0.198, 'neu': 0.728, 'pos': 0.074, 'compound': -0.9901},\n",
       " {'neg': 0.114, 'neu': 0.803, 'pos': 0.082, 'compound': -0.9454},\n",
       " {'neg': 0.204, 'neu': 0.726, 'pos': 0.069, 'compound': -0.9925},\n",
       " {'neg': 0.105, 'neu': 0.781, 'pos': 0.114, 'compound': 0.3102},\n",
       " {'neg': 0.104, 'neu': 0.825, 'pos': 0.071, 'compound': -0.8093},\n",
       " {'neg': 0.12, 'neu': 0.78, 'pos': 0.1, 'compound': -0.7458},\n",
       " {'neg': 0.129, 'neu': 0.798, 'pos': 0.073, 'compound': -0.8942},\n",
       " {'neg': 0.173, 'neu': 0.797, 'pos': 0.03, 'compound': -0.9909},\n",
       " {'neg': 0.201, 'neu': 0.77, 'pos': 0.029, 'compound': -0.998},\n",
       " {'neg': 0.161, 'neu': 0.761, 'pos': 0.078, 'compound': -0.9789},\n",
       " {'neg': 0.187, 'neu': 0.774, 'pos': 0.039, 'compound': -0.9954},\n",
       " {'neg': 0.106, 'neu': 0.814, 'pos': 0.079, 'compound': -0.7216},\n",
       " {'neg': 0.183, 'neu': 0.741, 'pos': 0.075, 'compound': -0.9842},\n",
       " {'neg': 0.159, 'neu': 0.784, 'pos': 0.057, 'compound': -0.9748},\n",
       " {'neg': 0.203, 'neu': 0.737, 'pos': 0.059, 'compound': -0.9776},\n",
       " {'neg': 0.13, 'neu': 0.804, 'pos': 0.066, 'compound': -0.9882},\n",
       " {'neg': 0.157, 'neu': 0.773, 'pos': 0.069, 'compound': -0.9871},\n",
       " {'neg': 0.173, 'neu': 0.786, 'pos': 0.041, 'compound': -0.9729},\n",
       " {'neg': 0.116, 'neu': 0.762, 'pos': 0.122, 'compound': 0.6659},\n",
       " {'neg': 0.117, 'neu': 0.79, 'pos': 0.094, 'compound': -0.9622},\n",
       " {'neg': 0.224, 'neu': 0.738, 'pos': 0.038, 'compound': -0.9859},\n",
       " {'neg': 0.131, 'neu': 0.801, 'pos': 0.068, 'compound': -0.9265},\n",
       " {'neg': 0.0, 'neu': 0.831, 'pos': 0.169, 'compound': 0.9899},\n",
       " {'neg': 0.137, 'neu': 0.817, 'pos': 0.045, 'compound': -0.9423},\n",
       " {'neg': 0.103, 'neu': 0.825, 'pos': 0.073, 'compound': -0.9417},\n",
       " {'neg': 0.1, 'neu': 0.837, 'pos': 0.063, 'compound': -0.8215},\n",
       " {'neg': 0.114, 'neu': 0.815, 'pos': 0.071, 'compound': -0.9049},\n",
       " {'neg': 0.062, 'neu': 0.834, 'pos': 0.104, 'compound': 0.8678},\n",
       " {'neg': 0.084, 'neu': 0.893, 'pos': 0.024, 'compound': -0.9753},\n",
       " {'neg': 0.081, 'neu': 0.824, 'pos': 0.095, 'compound': 0.5945},\n",
       " {'neg': 0.141, 'neu': 0.795, 'pos': 0.065, 'compound': -0.963},\n",
       " {'neg': 0.162, 'neu': 0.765, 'pos': 0.073, 'compound': -0.9341},\n",
       " {'neg': 0.055, 'neu': 0.821, 'pos': 0.124, 'compound': 0.9696},\n",
       " {'neg': 0.093, 'neu': 0.854, 'pos': 0.053, 'compound': -0.9471},\n",
       " {'neg': 0.038, 'neu': 0.862, 'pos': 0.1, 'compound': 0.9678},\n",
       " {'neg': 0.078, 'neu': 0.835, 'pos': 0.087, 'compound': 0.5994},\n",
       " {'neg': 0.123, 'neu': 0.734, 'pos': 0.143, 'compound': 0.7351},\n",
       " {'neg': 0.065, 'neu': 0.851, 'pos': 0.084, 'compound': 0.7807},\n",
       " {'neg': 0.133, 'neu': 0.77, 'pos': 0.097, 'compound': -0.9803},\n",
       " {'neg': 0.01, 'neu': 0.838, 'pos': 0.151, 'compound': 0.995},\n",
       " {'neg': 0.025, 'neu': 0.858, 'pos': 0.117, 'compound': 0.9948},\n",
       " {'neg': 0.025, 'neu': 0.858, 'pos': 0.116, 'compound': 0.987},\n",
       " {'neg': 0.182, 'neu': 0.744, 'pos': 0.073, 'compound': -0.9853},\n",
       " {'neg': 0.211, 'neu': 0.726, 'pos': 0.063, 'compound': -0.9923},\n",
       " {'neg': 0.199, 'neu': 0.752, 'pos': 0.049, 'compound': -0.9865},\n",
       " {'neg': 0.169, 'neu': 0.765, 'pos': 0.066, 'compound': -0.9871},\n",
       " {'neg': 0.171, 'neu': 0.754, 'pos': 0.075, 'compound': -0.9774},\n",
       " {'neg': 0.185, 'neu': 0.731, 'pos': 0.084, 'compound': -0.9755},\n",
       " {'neg': 0.07, 'neu': 0.836, 'pos': 0.094, 'compound': 0.8717},\n",
       " {'neg': 0.137, 'neu': 0.802, 'pos': 0.061, 'compound': -0.9965},\n",
       " {'neg': 0.093, 'neu': 0.81, 'pos': 0.097, 'compound': -0.6369},\n",
       " {'neg': 0.077, 'neu': 0.79, 'pos': 0.133, 'compound': 0.9887},\n",
       " {'neg': 0.026, 'neu': 0.92, 'pos': 0.054, 'compound': 0.8297},\n",
       " {'neg': 0.091, 'neu': 0.786, 'pos': 0.123, 'compound': 0.9538},\n",
       " {'neg': 0.14, 'neu': 0.798, 'pos': 0.062, 'compound': -0.99},\n",
       " {'neg': 0.165, 'neu': 0.755, 'pos': 0.08, 'compound': -0.9931},\n",
       " {'neg': 0.127, 'neu': 0.779, 'pos': 0.093, 'compound': -0.9868},\n",
       " {'neg': 0.132, 'neu': 0.809, 'pos': 0.059, 'compound': -0.9996},\n",
       " {'neg': 0.124, 'neu': 0.792, 'pos': 0.084, 'compound': -0.9834},\n",
       " {'neg': 0.188, 'neu': 0.776, 'pos': 0.036, 'compound': -0.9962},\n",
       " {'neg': 0.092, 'neu': 0.856, 'pos': 0.052, 'compound': -0.9967},\n",
       " {'neg': 0.203, 'neu': 0.693, 'pos': 0.104, 'compound': -0.9965},\n",
       " {'neg': 0.154, 'neu': 0.797, 'pos': 0.048, 'compound': -0.9859},\n",
       " {'neg': 0.143, 'neu': 0.823, 'pos': 0.033, 'compound': -0.9804},\n",
       " {'neg': 0.106, 'neu': 0.789, 'pos': 0.104, 'compound': -0.7657},\n",
       " {'neg': 0.107, 'neu': 0.787, 'pos': 0.107, 'compound': -0.7269},\n",
       " {'neg': 0.22, 'neu': 0.717, 'pos': 0.062, 'compound': -0.9872},\n",
       " {'neg': 0.143, 'neu': 0.773, 'pos': 0.083, 'compound': -0.9777},\n",
       " {'neg': 0.252, 'neu': 0.65, 'pos': 0.097, 'compound': -0.9958},\n",
       " {'neg': 0.212, 'neu': 0.704, 'pos': 0.084, 'compound': -0.9936},\n",
       " {'neg': 0.122, 'neu': 0.81, 'pos': 0.067, 'compound': -0.9656},\n",
       " {'neg': 0.128, 'neu': 0.763, 'pos': 0.109, 'compound': -0.8242},\n",
       " {'neg': 0.113, 'neu': 0.807, 'pos': 0.08, 'compound': -0.7927},\n",
       " {'neg': 0.137, 'neu': 0.769, 'pos': 0.094, 'compound': -0.9939},\n",
       " {'neg': 0.222, 'neu': 0.676, 'pos': 0.102, 'compound': -0.9886},\n",
       " {'neg': 0.163, 'neu': 0.779, 'pos': 0.058, 'compound': -0.9896},\n",
       " {'neg': 0.156, 'neu': 0.796, 'pos': 0.048, 'compound': -0.9816},\n",
       " {'neg': 0.092, 'neu': 0.831, 'pos': 0.077, 'compound': -0.5908},\n",
       " {'neg': 0.163, 'neu': 0.776, 'pos': 0.061, 'compound': -0.9955},\n",
       " {'neg': 0.167, 'neu': 0.802, 'pos': 0.031, 'compound': -0.9975},\n",
       " {'neg': 0.141, 'neu': 0.787, 'pos': 0.072, 'compound': -0.9579},\n",
       " {'neg': 0.165, 'neu': 0.724, 'pos': 0.111, 'compound': -0.9431},\n",
       " {'neg': 0.135, 'neu': 0.756, 'pos': 0.109, 'compound': -0.9527},\n",
       " {'neg': 0.184, 'neu': 0.738, 'pos': 0.078, 'compound': -0.9817},\n",
       " {'neg': 0.108, 'neu': 0.833, 'pos': 0.059, 'compound': -0.9265},\n",
       " {'neg': 0.155, 'neu': 0.758, 'pos': 0.087, 'compound': -0.9504},\n",
       " {'neg': 0.256, 'neu': 0.711, 'pos': 0.033, 'compound': -0.9889},\n",
       " {'neg': 0.191, 'neu': 0.765, 'pos': 0.044, 'compound': -0.9876},\n",
       " {'neg': 0.198, 'neu': 0.732, 'pos': 0.069, 'compound': -0.9928},\n",
       " {'neg': 0.145, 'neu': 0.763, 'pos': 0.092, 'compound': -0.9279},\n",
       " {'neg': 0.156, 'neu': 0.763, 'pos': 0.081, 'compound': -0.9614},\n",
       " {'neg': 0.216, 'neu': 0.717, 'pos': 0.067, 'compound': -0.9701},\n",
       " {'neg': 0.152, 'neu': 0.78, 'pos': 0.068, 'compound': -0.9894},\n",
       " {'neg': 0.186, 'neu': 0.777, 'pos': 0.037, 'compound': -0.9781},\n",
       " {'neg': 0.206, 'neu': 0.77, 'pos': 0.024, 'compound': -0.9986},\n",
       " {'neg': 0.126, 'neu': 0.788, 'pos': 0.086, 'compound': -0.9246},\n",
       " {'neg': 0.094, 'neu': 0.864, 'pos': 0.042, 'compound': -0.9766},\n",
       " {'neg': 0.098, 'neu': 0.807, 'pos': 0.095, 'compound': -0.3892},\n",
       " {'neg': 0.136, 'neu': 0.801, 'pos': 0.063, 'compound': -0.9748},\n",
       " {'neg': 0.121, 'neu': 0.8, 'pos': 0.08, 'compound': -0.875},\n",
       " {'neg': 0.115, 'neu': 0.799, 'pos': 0.086, 'compound': -0.8591},\n",
       " {'neg': 0.203, 'neu': 0.727, 'pos': 0.071, 'compound': -0.9954},\n",
       " {'neg': 0.122, 'neu': 0.824, 'pos': 0.054, 'compound': -0.9651},\n",
       " {'neg': 0.129, 'neu': 0.794, 'pos': 0.076, 'compound': -0.9239},\n",
       " {'neg': 0.154, 'neu': 0.802, 'pos': 0.045, 'compound': -0.9885},\n",
       " {'neg': 0.148, 'neu': 0.777, 'pos': 0.075, 'compound': -0.9597},\n",
       " {'neg': 0.133, 'neu': 0.81, 'pos': 0.057, 'compound': -0.984},\n",
       " {'neg': 0.149, 'neu': 0.775, 'pos': 0.077, 'compound': -0.9881},\n",
       " {'neg': 0.214, 'neu': 0.747, 'pos': 0.04, 'compound': -0.9956},\n",
       " {'neg': 0.114, 'neu': 0.864, 'pos': 0.021, 'compound': -0.9833},\n",
       " {'neg': 0.123, 'neu': 0.819, 'pos': 0.058, 'compound': -0.8934},\n",
       " {'neg': 0.112, 'neu': 0.824, 'pos': 0.064, 'compound': -0.9183},\n",
       " {'neg': 0.166, 'neu': 0.735, 'pos': 0.099, 'compound': -0.9873},\n",
       " {'neg': 0.114, 'neu': 0.822, 'pos': 0.064, 'compound': -0.9552},\n",
       " {'neg': 0.095, 'neu': 0.823, 'pos': 0.082, 'compound': -0.3818},\n",
       " {'neg': 0.131, 'neu': 0.735, 'pos': 0.134, 'compound': 0.6652},\n",
       " {'neg': 0.117, 'neu': 0.752, 'pos': 0.132, 'compound': 0.6486},\n",
       " {'neg': 0.143, 'neu': 0.78, 'pos': 0.077, 'compound': -0.9477},\n",
       " {'neg': 0.109, 'neu': 0.807, 'pos': 0.084, 'compound': -0.891},\n",
       " {'neg': 0.184, 'neu': 0.773, 'pos': 0.043, 'compound': -0.9873},\n",
       " {'neg': 0.115, 'neu': 0.844, 'pos': 0.041, 'compound': -0.9477},\n",
       " {'neg': 0.197, 'neu': 0.696, 'pos': 0.106, 'compound': -0.9947},\n",
       " {'neg': 0.153, 'neu': 0.794, 'pos': 0.053, 'compound': -0.9943},\n",
       " {'neg': 0.109, 'neu': 0.826, 'pos': 0.064, 'compound': -0.9295},\n",
       " {'neg': 0.191, 'neu': 0.756, 'pos': 0.052, 'compound': -0.9666},\n",
       " {'neg': 0.191, 'neu': 0.766, 'pos': 0.042, 'compound': -0.994},\n",
       " {'neg': 0.134, 'neu': 0.775, 'pos': 0.092, 'compound': -0.765},\n",
       " {'neg': 0.114, 'neu': 0.806, 'pos': 0.08, 'compound': -0.8471},\n",
       " {'neg': 0.203, 'neu': 0.719, 'pos': 0.078, 'compound': -0.9936},\n",
       " {'neg': 0.112, 'neu': 0.819, 'pos': 0.069, 'compound': -0.9728},\n",
       " {'neg': 0.173, 'neu': 0.737, 'pos': 0.091, 'compound': -0.9499},\n",
       " {'neg': 0.092, 'neu': 0.757, 'pos': 0.151, 'compound': 0.9901},\n",
       " {'neg': 0.175, 'neu': 0.778, 'pos': 0.047, 'compound': -0.9898},\n",
       " {'neg': 0.114, 'neu': 0.846, 'pos': 0.04, 'compound': -0.9868},\n",
       " {'neg': 0.2, 'neu': 0.754, 'pos': 0.046, 'compound': -0.985},\n",
       " {'neg': 0.177, 'neu': 0.779, 'pos': 0.044, 'compound': -0.9956},\n",
       " {'neg': 0.149, 'neu': 0.793, 'pos': 0.058, 'compound': -0.9965},\n",
       " {'neg': 0.196, 'neu': 0.759, 'pos': 0.045, 'compound': -0.9867},\n",
       " {'neg': 0.099, 'neu': 0.832, 'pos': 0.069, 'compound': -0.9492},\n",
       " {'neg': 0.123, 'neu': 0.811, 'pos': 0.066, 'compound': -0.989},\n",
       " {'neg': 0.159, 'neu': 0.779, 'pos': 0.062, 'compound': -0.9973},\n",
       " {'neg': 0.229, 'neu': 0.713, 'pos': 0.058, 'compound': -0.9991},\n",
       " {'neg': 0.196, 'neu': 0.746, 'pos': 0.058, 'compound': -0.9982},\n",
       " {'neg': 0.191, 'neu': 0.77, 'pos': 0.039, 'compound': -0.9944},\n",
       " {'neg': 0.134, 'neu': 0.762, 'pos': 0.104, 'compound': -0.9182},\n",
       " {'neg': 0.142, 'neu': 0.803, 'pos': 0.055, 'compound': -0.9802},\n",
       " {'neg': 0.086, 'neu': 0.805, 'pos': 0.109, 'compound': 0.3535},\n",
       " {'neg': 0.091, 'neu': 0.813, 'pos': 0.096, 'compound': -0.1621},\n",
       " {'neg': 0.224, 'neu': 0.686, 'pos': 0.09, 'compound': -0.9904},\n",
       " {'neg': 0.125, 'neu': 0.826, 'pos': 0.048, 'compound': -0.9201},\n",
       " {'neg': 0.117, 'neu': 0.814, 'pos': 0.069, 'compound': -0.9375},\n",
       " {'neg': 0.099, 'neu': 0.821, 'pos': 0.08, 'compound': -0.7458},\n",
       " {'neg': 0.107, 'neu': 0.847, 'pos': 0.045, 'compound': -0.9534},\n",
       " {'neg': 0.097, 'neu': 0.822, 'pos': 0.081, 'compound': -0.8532},\n",
       " {'neg': 0.146, 'neu': 0.785, 'pos': 0.07, 'compound': -0.9724},\n",
       " {'neg': 0.166, 'neu': 0.717, 'pos': 0.116, 'compound': -0.9488},\n",
       " {'neg': 0.207, 'neu': 0.743, 'pos': 0.05, 'compound': -0.9958},\n",
       " {'neg': 0.155, 'neu': 0.754, 'pos': 0.092, 'compound': -0.9774},\n",
       " {'neg': 0.193, 'neu': 0.743, 'pos': 0.065, 'compound': -0.9978},\n",
       " {'neg': 0.152, 'neu': 0.781, 'pos': 0.067, 'compound': -0.9893},\n",
       " {'neg': 0.202, 'neu': 0.722, 'pos': 0.076, 'compound': -0.9979},\n",
       " {'neg': 0.219, 'neu': 0.736, 'pos': 0.046, 'compound': -0.9879},\n",
       " {'neg': 0.227, 'neu': 0.677, 'pos': 0.096, 'compound': -0.9967},\n",
       " {'neg': 0.247, 'neu': 0.701, 'pos': 0.052, 'compound': -0.9973},\n",
       " {'neg': 0.277, 'neu': 0.675, 'pos': 0.048, 'compound': -0.998},\n",
       " {'neg': 0.168, 'neu': 0.756, 'pos': 0.076, 'compound': -0.9894},\n",
       " {'neg': 0.142, 'neu': 0.769, 'pos': 0.089, 'compound': -0.9699},\n",
       " {'neg': 0.102, 'neu': 0.818, 'pos': 0.081, 'compound': -0.8375},\n",
       " {'neg': 0.101, 'neu': 0.806, 'pos': 0.093, 'compound': -0.235},\n",
       " {'neg': 0.204, 'neu': 0.79, 'pos': 0.006, 'compound': -0.9952},\n",
       " {'neg': 0.086, 'neu': 0.787, 'pos': 0.127, 'compound': 0.9344},\n",
       " {'neg': 0.239, 'neu': 0.723, 'pos': 0.037, 'compound': -0.9961},\n",
       " {'neg': 0.233, 'neu': 0.737, 'pos': 0.031, 'compound': -0.9905},\n",
       " {'neg': 0.231, 'neu': 0.679, 'pos': 0.09, 'compound': -0.9906},\n",
       " {'neg': 0.216, 'neu': 0.706, 'pos': 0.077, 'compound': -0.9871},\n",
       " {'neg': 0.206, 'neu': 0.764, 'pos': 0.03, 'compound': -0.9921},\n",
       " {'neg': 0.192, 'neu': 0.778, 'pos': 0.03, 'compound': -0.9901},\n",
       " {'neg': 0.237, 'neu': 0.714, 'pos': 0.049, 'compound': -0.9917},\n",
       " {'neg': 0.13, 'neu': 0.801, 'pos': 0.069, 'compound': -0.9407},\n",
       " {'neg': 0.162, 'neu': 0.775, 'pos': 0.063, 'compound': -0.9869},\n",
       " {'neg': 0.091, 'neu': 0.827, 'pos': 0.082, 'compound': -0.4472},\n",
       " {'neg': 0.196, 'neu': 0.755, 'pos': 0.049, 'compound': -0.9623},\n",
       " {'neg': 0.23, 'neu': 0.749, 'pos': 0.021, 'compound': -0.9944},\n",
       " {'neg': 0.162, 'neu': 0.717, 'pos': 0.122, 'compound': -0.9709},\n",
       " {'neg': 0.125, 'neu': 0.786, 'pos': 0.089, 'compound': -0.9306},\n",
       " {'neg': 0.136, 'neu': 0.759, 'pos': 0.104, 'compound': -0.8409},\n",
       " {'neg': 0.128, 'neu': 0.799, 'pos': 0.073, 'compound': -0.8942},\n",
       " {'neg': 0.208, 'neu': 0.748, 'pos': 0.044, 'compound': -0.9913},\n",
       " {'neg': 0.123, 'neu': 0.797, 'pos': 0.081, 'compound': -0.9828},\n",
       " {'neg': 0.151, 'neu': 0.766, 'pos': 0.083, 'compound': -0.9274},\n",
       " {'neg': 0.202, 'neu': 0.724, 'pos': 0.074, 'compound': -0.9963},\n",
       " {'neg': 0.172, 'neu': 0.761, 'pos': 0.067, 'compound': -0.9976},\n",
       " {'neg': 0.2, 'neu': 0.743, 'pos': 0.057, 'compound': -0.9938},\n",
       " {'neg': 0.166, 'neu': 0.767, 'pos': 0.067, 'compound': -0.9828},\n",
       " {'neg': 0.18, 'neu': 0.74, 'pos': 0.08, 'compound': -0.9961},\n",
       " {'neg': 0.034, 'neu': 0.847, 'pos': 0.118, 'compound': 0.9915},\n",
       " {'neg': 0.072, 'neu': 0.768, 'pos': 0.161, 'compound': 0.9873},\n",
       " {'neg': 0.043, 'neu': 0.842, 'pos': 0.115, 'compound': 0.9842},\n",
       " {'neg': 0.054, 'neu': 0.772, 'pos': 0.174, 'compound': 0.9919},\n",
       " {'neg': 0.067, 'neu': 0.826, 'pos': 0.107, 'compound': 0.9469},\n",
       " {'neg': 0.062, 'neu': 0.87, 'pos': 0.068, 'compound': 0.2023},\n",
       " {'neg': 0.083, 'neu': 0.795, 'pos': 0.122, 'compound': 0.9913},\n",
       " {'neg': 0.049, 'neu': 0.838, 'pos': 0.113, 'compound': 0.9966},\n",
       " {'neg': 0.102, 'neu': 0.78, 'pos': 0.117, 'compound': 0.7379},\n",
       " {'neg': 0.012, 'neu': 0.869, 'pos': 0.119, 'compound': 0.989},\n",
       " {'neg': 0.064, 'neu': 0.824, 'pos': 0.111, 'compound': 0.9493},\n",
       " {'neg': 0.054, 'neu': 0.834, 'pos': 0.113, 'compound': 0.9712},\n",
       " {'neg': 0.027, 'neu': 0.894, 'pos': 0.078, 'compound': 0.9609},\n",
       " {'neg': 0.056, 'neu': 0.838, 'pos': 0.106, 'compound': 0.9901},\n",
       " {'neg': 0.046, 'neu': 0.821, 'pos': 0.132, 'compound': 0.998},\n",
       " {'neg': 0.067, 'neu': 0.853, 'pos': 0.08, 'compound': 0.1779},\n",
       " {'neg': 0.036, 'neu': 0.794, 'pos': 0.17, 'compound': 0.9968},\n",
       " {'neg': 0.112, 'neu': 0.804, 'pos': 0.083, 'compound': -0.8415},\n",
       " {'neg': 0.023, 'neu': 0.884, 'pos': 0.093, 'compound': 0.9932},\n",
       " {'neg': 0.015, 'neu': 0.788, 'pos': 0.197, 'compound': 0.9925},\n",
       " {'neg': 0.117, 'neu': 0.74, 'pos': 0.143, 'compound': 0.4781},\n",
       " {'neg': 0.068, 'neu': 0.881, 'pos': 0.05, 'compound': -0.7096},\n",
       " {'neg': 0.06, 'neu': 0.815, 'pos': 0.126, 'compound': 0.9862},\n",
       " {'neg': 0.093, 'neu': 0.822, 'pos': 0.085, 'compound': -0.4854},\n",
       " {'neg': 0.007, 'neu': 0.797, 'pos': 0.196, 'compound': 0.994},\n",
       " {'neg': 0.103, 'neu': 0.704, 'pos': 0.193, 'compound': 0.9846},\n",
       " {'neg': 0.118, 'neu': 0.809, 'pos': 0.072, 'compound': -0.9377},\n",
       " {'neg': 0.15, 'neu': 0.774, 'pos': 0.076, 'compound': -0.9929},\n",
       " {'neg': 0.084, 'neu': 0.845, 'pos': 0.071, 'compound': -0.8507},\n",
       " {'neg': 0.091, 'neu': 0.831, 'pos': 0.079, 'compound': -0.864},\n",
       " {'neg': 0.169, 'neu': 0.754, 'pos': 0.077, 'compound': -0.9962},\n",
       " {'neg': 0.074, 'neu': 0.86, 'pos': 0.066, 'compound': -0.3777},\n",
       " {'neg': 0.091, 'neu': 0.858, 'pos': 0.052, 'compound': -0.8151},\n",
       " {'neg': 0.089, 'neu': 0.85, 'pos': 0.061, 'compound': -0.8388},\n",
       " {'neg': 0.105, 'neu': 0.775, 'pos': 0.121, 'compound': 0.7126},\n",
       " {'neg': 0.079, 'neu': 0.828, 'pos': 0.093, 'compound': 0.7096},\n",
       " {'neg': 0.085, 'neu': 0.814, 'pos': 0.101, 'compound': 0.6365},\n",
       " {'neg': 0.064, 'neu': 0.89, 'pos': 0.047, 'compound': -0.5106},\n",
       " {'neg': 0.07, 'neu': 0.903, 'pos': 0.027, 'compound': -0.9584},\n",
       " {'neg': 0.122, 'neu': 0.777, 'pos': 0.101, 'compound': -0.8865},\n",
       " {'neg': 0.11, 'neu': 0.826, 'pos': 0.064, 'compound': -0.9674},\n",
       " {'neg': 0.163, 'neu': 0.791, 'pos': 0.046, 'compound': -0.9895},\n",
       " {'neg': 0.175, 'neu': 0.762, 'pos': 0.063, 'compound': -0.9949},\n",
       " {'neg': 0.157, 'neu': 0.793, 'pos': 0.05, 'compound': -0.993},\n",
       " {'neg': 0.186, 'neu': 0.745, 'pos': 0.069, 'compound': -0.9887},\n",
       " {'neg': 0.05, 'neu': 0.874, 'pos': 0.076, 'compound': 0.8313},\n",
       " {'neg': 0.093, 'neu': 0.819, 'pos': 0.088, 'compound': -0.3167},\n",
       " {'neg': 0.086, 'neu': 0.826, 'pos': 0.088, 'compound': 0.2023},\n",
       " {'neg': 0.085, 'neu': 0.775, 'pos': 0.14, 'compound': 0.9488},\n",
       " {'neg': 0.084, 'neu': 0.828, 'pos': 0.088, 'compound': 0.501},\n",
       " {'neg': 0.059, 'neu': 0.846, 'pos': 0.095, 'compound': 0.8878},\n",
       " {'neg': 0.116, 'neu': 0.766, 'pos': 0.118, 'compound': 0.2579},\n",
       " {'neg': 0.098, 'neu': 0.779, 'pos': 0.122, 'compound': 0.8555},\n",
       " {'neg': 0.042, 'neu': 0.891, 'pos': 0.067, 'compound': 0.8986},\n",
       " {'neg': 0.07, 'neu': 0.808, 'pos': 0.123, 'compound': 0.9633},\n",
       " {'neg': 0.102, 'neu': 0.832, 'pos': 0.065, 'compound': -0.9164},\n",
       " {'neg': 0.039, 'neu': 0.884, 'pos': 0.076, 'compound': 0.81},\n",
       " {'neg': 0.033, 'neu': 0.927, 'pos': 0.04, 'compound': 0.4359},\n",
       " {'neg': 0.09, 'neu': 0.855, 'pos': 0.055, 'compound': -0.979},\n",
       " {'neg': 0.085, 'neu': 0.847, 'pos': 0.068, 'compound': -0.8372},\n",
       " {'neg': 0.07, 'neu': 0.88, 'pos': 0.049, 'compound': -0.6486},\n",
       " {'neg': 0.092, 'neu': 0.859, 'pos': 0.049, 'compound': -0.9493},\n",
       " {'neg': 0.126, 'neu': 0.782, 'pos': 0.092, 'compound': -0.9279},\n",
       " {'neg': 0.094, 'neu': 0.751, 'pos': 0.155, 'compound': 0.9433},\n",
       " {'neg': 0.044, 'neu': 0.866, 'pos': 0.09, 'compound': 0.9606},\n",
       " {'neg': 0.029, 'neu': 0.87, 'pos': 0.101, 'compound': 0.9794},\n",
       " {'neg': 0.092, 'neu': 0.795, 'pos': 0.114, 'compound': 0.9578},\n",
       " {'neg': 0.049, 'neu': 0.79, 'pos': 0.162, 'compound': 0.9964},\n",
       " {'neg': 0.023, 'neu': 0.751, 'pos': 0.226, 'compound': 0.9966},\n",
       " {'neg': 0.052, 'neu': 0.775, 'pos': 0.173, 'compound': 0.9937},\n",
       " {'neg': 0.074, 'neu': 0.832, 'pos': 0.094, 'compound': 0.5574},\n",
       " {'neg': 0.023, 'neu': 0.787, 'pos': 0.19, 'compound': 0.9938},\n",
       " {'neg': 0.006, 'neu': 0.893, 'pos': 0.101, 'compound': 0.986},\n",
       " {'neg': 0.012, 'neu': 0.799, 'pos': 0.189, 'compound': 0.9968},\n",
       " {'neg': 0.006, 'neu': 0.806, 'pos': 0.188, 'compound': 0.9959},\n",
       " {'neg': 0.067, 'neu': 0.827, 'pos': 0.106, 'compound': 0.9217},\n",
       " {'neg': 0.054, 'neu': 0.86, 'pos': 0.086, 'compound': 0.9241},\n",
       " {'neg': 0.009, 'neu': 0.832, 'pos': 0.159, 'compound': 0.9888},\n",
       " {'neg': 0.021, 'neu': 0.832, 'pos': 0.148, 'compound': 0.9884},\n",
       " {'neg': 0.0, 'neu': 0.85, 'pos': 0.15, 'compound': 0.9758},\n",
       " {'neg': 0.014, 'neu': 0.831, 'pos': 0.155, 'compound': 0.9868},\n",
       " {'neg': 0.048, 'neu': 0.843, 'pos': 0.11, 'compound': 0.9831},\n",
       " {'neg': 0.11, 'neu': 0.73, 'pos': 0.16, 'compound': 0.9286},\n",
       " {'neg': 0.068, 'neu': 0.748, 'pos': 0.184, 'compound': 0.9917},\n",
       " {'neg': 0.04, 'neu': 0.799, 'pos': 0.161, 'compound': 0.9973},\n",
       " {'neg': 0.037, 'neu': 0.841, 'pos': 0.121, 'compound': 0.9957},\n",
       " {'neg': 0.026, 'neu': 0.87, 'pos': 0.103, 'compound': 0.9936},\n",
       " {'neg': 0.133, 'neu': 0.69, 'pos': 0.178, 'compound': 0.6486},\n",
       " {'neg': 0.02, 'neu': 0.824, 'pos': 0.156, 'compound': 0.9858},\n",
       " {'neg': 0.077, 'neu': 0.793, 'pos': 0.13, 'compound': 0.9842},\n",
       " {'neg': 0.068, 'neu': 0.779, 'pos': 0.153, 'compound': 0.9929},\n",
       " {'neg': 0.023, 'neu': 0.805, 'pos': 0.171, 'compound': 0.9924},\n",
       " {'neg': 0.04, 'neu': 0.863, 'pos': 0.097, 'compound': 0.936},\n",
       " {'neg': 0.1, 'neu': 0.796, 'pos': 0.104, 'compound': -0.9391},\n",
       " {'neg': 0.072, 'neu': 0.817, 'pos': 0.112, 'compound': 0.9652},\n",
       " {'neg': 0.066, 'neu': 0.761, 'pos': 0.174, 'compound': 0.9928},\n",
       " {'neg': 0.006, 'neu': 0.838, 'pos': 0.156, 'compound': 0.9935},\n",
       " {'neg': 0.055, 'neu': 0.805, 'pos': 0.14, 'compound': 0.9927},\n",
       " {'neg': 0.14, 'neu': 0.648, 'pos': 0.213, 'compound': 0.9882},\n",
       " {'neg': 0.062, 'neu': 0.829, 'pos': 0.11, 'compound': 0.969},\n",
       " {'neg': 0.126, 'neu': 0.75, 'pos': 0.124, 'compound': -0.872},\n",
       " {'neg': 0.0, 'neu': 0.856, 'pos': 0.144, 'compound': 0.9766},\n",
       " {'neg': 0.029, 'neu': 0.866, 'pos': 0.105, 'compound': 0.9766},\n",
       " {'neg': 0.043, 'neu': 0.857, 'pos': 0.1, 'compound': 0.9427},\n",
       " {'neg': 0.032, 'neu': 0.89, 'pos': 0.077, 'compound': 0.9118},\n",
       " {'neg': 0.007, 'neu': 0.948, 'pos': 0.046, 'compound': 0.9348},\n",
       " {'neg': 0.079, 'neu': 0.88, 'pos': 0.041, 'compound': -0.8968},\n",
       " {'neg': 0.111, 'neu': 0.784, 'pos': 0.105, 'compound': -0.5859},\n",
       " {'neg': 0.098, 'neu': 0.758, 'pos': 0.144, 'compound': 0.802},\n",
       " {'neg': 0.065, 'neu': 0.864, 'pos': 0.071, 'compound': 0.4019},\n",
       " {'neg': 0.062, 'neu': 0.828, 'pos': 0.111, 'compound': 0.9538},\n",
       " {'neg': 0.031, 'neu': 0.849, 'pos': 0.12, 'compound': 0.9799},\n",
       " {'neg': 0.042, 'neu': 0.897, 'pos': 0.061, 'compound': 0.6896},\n",
       " {'neg': 0.087, 'neu': 0.859, 'pos': 0.054, 'compound': -0.9217},\n",
       " {'neg': 0.051, 'neu': 0.81, 'pos': 0.139, 'compound': 0.9771},\n",
       " {'neg': 0.06, 'neu': 0.848, 'pos': 0.091, 'compound': 0.5023},\n",
       " {'neg': 0.059, 'neu': 0.817, 'pos': 0.124, 'compound': 0.976},\n",
       " {'neg': 0.039, 'neu': 0.891, 'pos': 0.07, 'compound': 0.9217},\n",
       " {'neg': 0.085, 'neu': 0.823, 'pos': 0.092, 'compound': 0.4404},\n",
       " {'neg': 0.055, 'neu': 0.822, 'pos': 0.124, 'compound': 0.9895},\n",
       " {'neg': 0.013, 'neu': 0.83, 'pos': 0.157, 'compound': 0.9904},\n",
       " {'neg': 0.093, 'neu': 0.839, 'pos': 0.067, 'compound': -0.8781},\n",
       " {'neg': 0.12, 'neu': 0.793, 'pos': 0.088, 'compound': -0.8074},\n",
       " {'neg': 0.089, 'neu': 0.853, 'pos': 0.058, 'compound': -0.8962},\n",
       " {'neg': 0.052, 'neu': 0.876, 'pos': 0.071, 'compound': 0.836},\n",
       " {'neg': 0.055, 'neu': 0.885, 'pos': 0.06, 'compound': 0.6808},\n",
       " {'neg': 0.042, 'neu': 0.808, 'pos': 0.151, 'compound': 0.9855},\n",
       " {'neg': 0.042, 'neu': 0.909, 'pos': 0.049, 'compound': 0.3818},\n",
       " {'neg': 0.062, 'neu': 0.889, 'pos': 0.049, 'compound': -0.7391},\n",
       " {'neg': 0.053, 'neu': 0.876, 'pos': 0.071, 'compound': 0.6597},\n",
       " {'neg': 0.066, 'neu': 0.863, 'pos': 0.071, 'compound': 0.34},\n",
       " {'neg': 0.051, 'neu': 0.894, 'pos': 0.055, 'compound': -0.4322},\n",
       " {'neg': 0.076, 'neu': 0.843, 'pos': 0.081, 'compound': 0.1484},\n",
       " {'neg': 0.077, 'neu': 0.856, 'pos': 0.066, 'compound': -0.483},\n",
       " {'neg': 0.081, 'neu': 0.813, 'pos': 0.106, 'compound': 0.7391},\n",
       " {'neg': 0.071, 'neu': 0.862, 'pos': 0.067, 'compound': -0.128},\n",
       " {'neg': 0.135, 'neu': 0.771, 'pos': 0.093, 'compound': -0.9081},\n",
       " {'neg': 0.013, 'neu': 0.809, 'pos': 0.178, 'compound': 0.996},\n",
       " {'neg': 0.026, 'neu': 0.848, 'pos': 0.126, 'compound': 0.959},\n",
       " {'neg': 0.065, 'neu': 0.849, 'pos': 0.086, 'compound': 0.9505},\n",
       " {'neg': 0.046, 'neu': 0.842, 'pos': 0.112, 'compound': 0.9791},\n",
       " {'neg': 0.059, 'neu': 0.856, 'pos': 0.084, 'compound': 0.8074},\n",
       " {'neg': 0.098, 'neu': 0.799, 'pos': 0.104, 'compound': -0.0516},\n",
       " {'neg': 0.102, 'neu': 0.843, 'pos': 0.055, 'compound': -0.9735},\n",
       " {'neg': 0.105, 'neu': 0.805, 'pos': 0.09, 'compound': -0.5574},\n",
       " {'neg': 0.163, 'neu': 0.797, 'pos': 0.04, 'compound': -0.9963},\n",
       " {'neg': 0.095, 'neu': 0.842, 'pos': 0.062, 'compound': -0.9272},\n",
       " {'neg': 0.0, 'neu': 0.814, 'pos': 0.186, 'compound': 0.9851},\n",
       " {'neg': 0.081, 'neu': 0.91, 'pos': 0.009, 'compound': -0.9753},\n",
       " {'neg': 0.05, 'neu': 0.863, 'pos': 0.086, 'compound': 0.9827},\n",
       " {'neg': 0.131, 'neu': 0.803, 'pos': 0.066, 'compound': -0.975},\n",
       " {'neg': 0.083, 'neu': 0.876, 'pos': 0.041, 'compound': -0.9618},\n",
       " {'neg': 0.185, 'neu': 0.682, 'pos': 0.133, 'compound': -0.991},\n",
       " {'neg': 0.1, 'neu': 0.84, 'pos': 0.06, 'compound': -0.9062},\n",
       " {'neg': 0.084, 'neu': 0.863, 'pos': 0.052, 'compound': -0.8176},\n",
       " {'neg': 0.099, 'neu': 0.829, 'pos': 0.073, 'compound': -0.952},\n",
       " {'neg': 0.205, 'neu': 0.7, 'pos': 0.095, 'compound': -0.9944},\n",
       " {'neg': 0.068, 'neu': 0.825, 'pos': 0.108, 'compound': 0.9382},\n",
       " {'neg': 0.112, 'neu': 0.765, 'pos': 0.123, 'compound': -0.4556},\n",
       " {'neg': 0.151, 'neu': 0.74, 'pos': 0.109, 'compound': -0.7923},\n",
       " {'neg': 0.114, 'neu': 0.812, 'pos': 0.074, 'compound': -0.8568},\n",
       " {'neg': 0.099, 'neu': 0.813, 'pos': 0.088, 'compound': -0.6412},\n",
       " {'neg': 0.194, 'neu': 0.72, 'pos': 0.086, 'compound': -0.9982},\n",
       " {'neg': 0.156, 'neu': 0.802, 'pos': 0.042, 'compound': -0.9854},\n",
       " {'neg': 0.188, 'neu': 0.766, 'pos': 0.046, 'compound': -0.9945},\n",
       " {'neg': 0.119, 'neu': 0.789, 'pos': 0.092, 'compound': -0.9108},\n",
       " {'neg': 0.131, 'neu': 0.796, 'pos': 0.073, 'compound': -0.977},\n",
       " {'neg': 0.17, 'neu': 0.71, 'pos': 0.12, 'compound': -0.9805},\n",
       " {'neg': 0.072, 'neu': 0.863, 'pos': 0.065, 'compound': -0.6602},\n",
       " {'neg': 0.077, 'neu': 0.827, 'pos': 0.097, 'compound': 0.83},\n",
       " {'neg': 0.184, 'neu': 0.75, 'pos': 0.066, 'compound': -0.9926},\n",
       " {'neg': 0.194, 'neu': 0.738, 'pos': 0.068, 'compound': -0.9912},\n",
       " {'neg': 0.087, 'neu': 0.872, 'pos': 0.041, 'compound': -0.9509},\n",
       " {'neg': 0.169, 'neu': 0.806, 'pos': 0.025, 'compound': -0.9959},\n",
       " {'neg': 0.237, 'neu': 0.696, 'pos': 0.068, 'compound': -0.9978},\n",
       " {'neg': 0.111, 'neu': 0.881, 'pos': 0.008, 'compound': -0.9565},\n",
       " {'neg': 0.236, 'neu': 0.721, 'pos': 0.043, 'compound': -0.9865},\n",
       " {'neg': 0.162, 'neu': 0.732, 'pos': 0.106, 'compound': -0.9648},\n",
       " {'neg': 0.131, 'neu': 0.8, 'pos': 0.069, 'compound': -0.9618},\n",
       " {'neg': 0.181, 'neu': 0.746, 'pos': 0.073, 'compound': -0.9882},\n",
       " {'neg': 0.178, 'neu': 0.754, 'pos': 0.067, 'compound': -0.9852},\n",
       " {'neg': 0.106, 'neu': 0.806, 'pos': 0.088, 'compound': -0.7458},\n",
       " {'neg': 0.136, 'neu': 0.816, 'pos': 0.048, 'compound': -0.9899},\n",
       " {'neg': 0.195, 'neu': 0.687, 'pos': 0.118, 'compound': -0.9716},\n",
       " {'neg': 0.164, 'neu': 0.76, 'pos': 0.076, 'compound': -0.9766},\n",
       " {'neg': 0.217, 'neu': 0.733, 'pos': 0.05, 'compound': -0.9788},\n",
       " {'neg': 0.169, 'neu': 0.776, 'pos': 0.056, 'compound': -0.9932},\n",
       " {'neg': 0.148, 'neu': 0.786, 'pos': 0.066, 'compound': -0.9652},\n",
       " {'neg': 0.168, 'neu': 0.77, 'pos': 0.062, 'compound': -0.987},\n",
       " {'neg': 0.118, 'neu': 0.775, 'pos': 0.107, 'compound': -0.6845},\n",
       " {'neg': 0.172, 'neu': 0.759, 'pos': 0.069, 'compound': -0.993},\n",
       " {'neg': 0.137, 'neu': 0.805, 'pos': 0.057, 'compound': -0.9867},\n",
       " {'neg': 0.143, 'neu': 0.815, 'pos': 0.043, 'compound': -0.9843},\n",
       " {'neg': 0.186, 'neu': 0.741, 'pos': 0.073, 'compound': -0.9884},\n",
       " {'neg': 0.214, 'neu': 0.732, 'pos': 0.054, 'compound': -0.9709},\n",
       " {'neg': 0.176, 'neu': 0.77, 'pos': 0.054, 'compound': -0.9862},\n",
       " {'neg': 0.17, 'neu': 0.75, 'pos': 0.081, 'compound': -0.995},\n",
       " {'neg': 0.204, 'neu': 0.706, 'pos': 0.089, 'compound': -0.9979},\n",
       " {'neg': 0.2, 'neu': 0.769, 'pos': 0.03, 'compound': -0.9906},\n",
       " {'neg': 0.102, 'neu': 0.827, 'pos': 0.071, 'compound': -0.8242},\n",
       " {'neg': 0.077, 'neu': 0.823, 'pos': 0.1, 'compound': 0.6077},\n",
       " {'neg': 0.193, 'neu': 0.71, 'pos': 0.096, 'compound': -0.9808},\n",
       " {'neg': 0.165, 'neu': 0.753, 'pos': 0.082, 'compound': -0.9519},\n",
       " {'neg': 0.188, 'neu': 0.706, 'pos': 0.106, 'compound': -0.9652},\n",
       " {'neg': 0.11, 'neu': 0.803, 'pos': 0.087, 'compound': -0.8701},\n",
       " {'neg': 0.113, 'neu': 0.804, 'pos': 0.082, 'compound': -0.73},\n",
       " {'neg': 0.132, 'neu': 0.789, 'pos': 0.079, 'compound': -0.9386},\n",
       " {'neg': 0.134, 'neu': 0.769, 'pos': 0.097, 'compound': -0.8919},\n",
       " {'neg': 0.119, 'neu': 0.873, 'pos': 0.008, 'compound': -0.9451},\n",
       " {'neg': 0.14, 'neu': 0.782, 'pos': 0.079, 'compound': -0.8919},\n",
       " {'neg': 0.165, 'neu': 0.732, 'pos': 0.102, 'compound': -0.9531},\n",
       " {'neg': 0.12, 'neu': 0.81, 'pos': 0.07, 'compound': -0.8987},\n",
       " {'neg': 0.131, 'neu': 0.794, 'pos': 0.074, 'compound': -0.8965},\n",
       " {'neg': 0.109, 'neu': 0.811, 'pos': 0.08, 'compound': -0.9469},\n",
       " {'neg': 0.096, 'neu': 0.803, 'pos': 0.101, 'compound': 0.0679},\n",
       " {'neg': 0.158, 'neu': 0.761, 'pos': 0.081, 'compound': -0.9949},\n",
       " {'neg': 0.194, 'neu': 0.741, 'pos': 0.064, 'compound': -0.9826},\n",
       " {'neg': 0.1, 'neu': 0.789, 'pos': 0.111, 'compound': -0.0865},\n",
       " {'neg': 0.098, 'neu': 0.791, 'pos': 0.111, 'compound': -0.0287},\n",
       " {'neg': 0.112, 'neu': 0.811, 'pos': 0.077, 'compound': -0.9159},\n",
       " {'neg': 0.104, 'neu': 0.791, 'pos': 0.105, 'compound': -0.5},\n",
       " {'neg': 0.097, 'neu': 0.797, 'pos': 0.107, 'compound': 0.0422},\n",
       " {'neg': 0.131, 'neu': 0.784, 'pos': 0.086, 'compound': -0.9504},\n",
       " {'neg': 0.245, 'neu': 0.715, 'pos': 0.04, 'compound': -0.9881},\n",
       " {'neg': 0.145, 'neu': 0.771, 'pos': 0.085, 'compound': -0.9943},\n",
       " {'neg': 0.144, 'neu': 0.769, 'pos': 0.088, 'compound': -0.9765},\n",
       " {'neg': 0.148, 'neu': 0.773, 'pos': 0.079, 'compound': -0.988},\n",
       " {'neg': 0.146, 'neu': 0.737, 'pos': 0.117, 'compound': -0.9178},\n",
       " {'neg': 0.042, 'neu': 0.768, 'pos': 0.189, 'compound': 0.9939},\n",
       " {'neg': 0.227, 'neu': 0.708, 'pos': 0.065, 'compound': -0.9882},\n",
       " {'neg': 0.101, 'neu': 0.83, 'pos': 0.069, 'compound': -0.8637},\n",
       " {'neg': 0.136, 'neu': 0.763, 'pos': 0.101, 'compound': -0.8637},\n",
       " {'neg': 0.185, 'neu': 0.743, 'pos': 0.073, 'compound': -0.983},\n",
       " {'neg': 0.189, 'neu': 0.695, 'pos': 0.115, 'compound': -0.982},\n",
       " {'neg': 0.172, 'neu': 0.714, 'pos': 0.114, 'compound': -0.9407},\n",
       " {'neg': 0.098, 'neu': 0.823, 'pos': 0.079, 'compound': -0.7631},\n",
       " {'neg': 0.208, 'neu': 0.74, 'pos': 0.052, 'compound': -0.9934},\n",
       " {'neg': 0.153, 'neu': 0.786, 'pos': 0.061, 'compound': -0.9663},\n",
       " {'neg': 0.227, 'neu': 0.692, 'pos': 0.081, 'compound': -0.9956},\n",
       " {'neg': 0.209, 'neu': 0.728, 'pos': 0.062, 'compound': -0.9942},\n",
       " {'neg': 0.222, 'neu': 0.702, 'pos': 0.076, 'compound': -0.9926},\n",
       " {'neg': 0.049, 'neu': 0.889, 'pos': 0.062, 'compound': 0.7311},\n",
       " {'neg': 0.175, 'neu': 0.795, 'pos': 0.03, 'compound': -0.9815},\n",
       " {'neg': 0.22, 'neu': 0.73, 'pos': 0.05, 'compound': -0.9986},\n",
       " {'neg': 0.219, 'neu': 0.722, 'pos': 0.059, 'compound': -0.9988},\n",
       " {'neg': 0.167, 'neu': 0.75, 'pos': 0.083, 'compound': -0.9848},\n",
       " {'neg': 0.202, 'neu': 0.725, 'pos': 0.073, 'compound': -0.9944},\n",
       " {'neg': 0.169, 'neu': 0.761, 'pos': 0.07, 'compound': -0.9864},\n",
       " {'neg': 0.156, 'neu': 0.735, 'pos': 0.108, 'compound': -0.9737},\n",
       " {'neg': 0.179, 'neu': 0.737, 'pos': 0.084, 'compound': -0.9912},\n",
       " {'neg': 0.182, 'neu': 0.741, 'pos': 0.077, 'compound': -0.9906},\n",
       " {'neg': 0.21, 'neu': 0.717, 'pos': 0.073, 'compound': -0.9952},\n",
       " {'neg': 0.149, 'neu': 0.754, 'pos': 0.097, 'compound': -0.9638},\n",
       " {'neg': 0.14, 'neu': 0.76, 'pos': 0.1, 'compound': -0.9118},\n",
       " {'neg': 0.121, 'neu': 0.823, 'pos': 0.056, 'compound': -0.9524},\n",
       " {'neg': 0.148, 'neu': 0.789, 'pos': 0.063, 'compound': -0.9702},\n",
       " {'neg': 0.106, 'neu': 0.883, 'pos': 0.011, 'compound': -0.946},\n",
       " {'neg': 0.138, 'neu': 0.791, 'pos': 0.071, 'compound': -0.9445},\n",
       " {'neg': 0.14, 'neu': 0.791, 'pos': 0.068, 'compound': -0.9592},\n",
       " {'neg': 0.101, 'neu': 0.832, 'pos': 0.067, 'compound': -0.7741},\n",
       " {'neg': 0.145, 'neu': 0.788, 'pos': 0.067, 'compound': -0.9519},\n",
       " {'neg': 0.133, 'neu': 0.782, 'pos': 0.085, 'compound': -0.8568},\n",
       " {'neg': 0.125, 'neu': 0.787, 'pos': 0.088, 'compound': -0.8568},\n",
       " {'neg': 0.181, 'neu': 0.752, 'pos': 0.067, 'compound': -0.9824},\n",
       " {'neg': 0.186, 'neu': 0.766, 'pos': 0.049, 'compound': -0.9936},\n",
       " {'neg': 0.164, 'neu': 0.75, 'pos': 0.086, 'compound': -0.9903},\n",
       " {'neg': 0.189, 'neu': 0.726, 'pos': 0.085, 'compound': -0.9965},\n",
       " {'neg': 0.18, 'neu': 0.724, 'pos': 0.097, 'compound': -0.9959},\n",
       " {'neg': 0.242, 'neu': 0.707, 'pos': 0.051, 'compound': -0.992},\n",
       " {'neg': 0.196, 'neu': 0.772, 'pos': 0.032, 'compound': -0.9946},\n",
       " {'neg': 0.137, 'neu': 0.763, 'pos': 0.099, 'compound': -0.9389},\n",
       " {'neg': 0.146, 'neu': 0.767, 'pos': 0.087, 'compound': -0.9279},\n",
       " {'neg': 0.205, 'neu': 0.728, 'pos': 0.067, 'compound': -0.9766},\n",
       " {'neg': 0.165, 'neu': 0.759, 'pos': 0.076, 'compound': -0.8225},\n",
       " {'neg': 0.182, 'neu': 0.742, 'pos': 0.076, 'compound': -0.9795},\n",
       " {'neg': 0.092, 'neu': 0.852, 'pos': 0.057, 'compound': -0.8724},\n",
       " {'neg': 0.181, 'neu': 0.748, 'pos': 0.071, 'compound': -0.9823},\n",
       " {'neg': 0.127, 'neu': 0.791, 'pos': 0.082, 'compound': -0.9779},\n",
       " {'neg': 0.133, 'neu': 0.782, 'pos': 0.084, 'compound': -0.9512},\n",
       " {'neg': 0.184, 'neu': 0.75, 'pos': 0.065, 'compound': -0.9895},\n",
       " {'neg': 0.18, 'neu': 0.736, 'pos': 0.084, 'compound': -0.9812},\n",
       " {'neg': 0.161, 'neu': 0.758, 'pos': 0.081, 'compound': -0.9969},\n",
       " {'neg': 0.121, 'neu': 0.806, 'pos': 0.073, 'compound': -0.9191},\n",
       " {'neg': 0.129, 'neu': 0.838, 'pos': 0.033, 'compound': -0.9823},\n",
       " {'neg': 0.142, 'neu': 0.801, 'pos': 0.057, 'compound': -0.9524},\n",
       " {'neg': 0.102, 'neu': 0.808, 'pos': 0.091, 'compound': -0.4472},\n",
       " {'neg': 0.153, 'neu': 0.72, 'pos': 0.127, 'compound': -0.8826},\n",
       " {'neg': 0.209, 'neu': 0.734, 'pos': 0.057, 'compound': -0.9915},\n",
       " {'neg': 0.153, 'neu': 0.75, 'pos': 0.097, 'compound': -0.9891},\n",
       " {'neg': 0.134, 'neu': 0.783, 'pos': 0.083, 'compound': -0.8987},\n",
       " {'neg': 0.108, 'neu': 0.818, 'pos': 0.074, 'compound': -0.8417},\n",
       " {'neg': 0.17, 'neu': 0.747, 'pos': 0.082, 'compound': -0.991},\n",
       " {'neg': 0.161, 'neu': 0.759, 'pos': 0.08, 'compound': -0.979},\n",
       " {'neg': 0.152, 'neu': 0.755, 'pos': 0.093, 'compound': -0.9191},\n",
       " {'neg': 0.123, 'neu': 0.756, 'pos': 0.122, 'compound': -0.3041},\n",
       " {'neg': 0.18, 'neu': 0.747, 'pos': 0.073, 'compound': -0.9468},\n",
       " {'neg': 0.149, 'neu': 0.815, 'pos': 0.036, 'compound': -0.9893},\n",
       " {'neg': 0.169, 'neu': 0.801, 'pos': 0.03, 'compound': -0.993},\n",
       " {'neg': 0.1, 'neu': 0.798, 'pos': 0.103, 'compound': -0.0865},\n",
       " {'neg': 0.121, 'neu': 0.785, 'pos': 0.094, 'compound': -0.9069},\n",
       " {'neg': 0.126, 'neu': 0.837, 'pos': 0.036, 'compound': -0.9853},\n",
       " {'neg': 0.133, 'neu': 0.806, 'pos': 0.06, 'compound': -0.9797},\n",
       " {'neg': 0.219, 'neu': 0.745, 'pos': 0.036, 'compound': -0.9893},\n",
       " {'neg': 0.205, 'neu': 0.767, 'pos': 0.027, 'compound': -0.9924},\n",
       " {'neg': 0.174, 'neu': 0.756, 'pos': 0.07, 'compound': -0.9806},\n",
       " {'neg': 0.142, 'neu': 0.804, 'pos': 0.054, 'compound': -0.9891},\n",
       " {'neg': 0.0, 'neu': 0.952, 'pos': 0.048, 'compound': 0.8481},\n",
       " {'neg': 0.106, 'neu': 0.813, 'pos': 0.081, 'compound': -0.9142},\n",
       " {'neg': 0.111, 'neu': 0.804, 'pos': 0.085, 'compound': -0.8894},\n",
       " {'neg': 0.117, 'neu': 0.828, 'pos': 0.055, 'compound': -0.9603},\n",
       " {'neg': 0.133, 'neu': 0.817, 'pos': 0.049, 'compound': -0.9775},\n",
       " {'neg': 0.11, 'neu': 0.833, 'pos': 0.058, 'compound': -0.9142},\n",
       " {'neg': 0.107, 'neu': 0.816, 'pos': 0.077, 'compound': -0.6637},\n",
       " {'neg': 0.206, 'neu': 0.709, 'pos': 0.086, 'compound': -0.9959},\n",
       " {'neg': 0.188, 'neu': 0.749, 'pos': 0.063, 'compound': -0.9975},\n",
       " {'neg': 0.189, 'neu': 0.748, 'pos': 0.063, 'compound': -0.9847},\n",
       " {'neg': 0.167, 'neu': 0.766, 'pos': 0.066, 'compound': -0.9957},\n",
       " {'neg': 0.185, 'neu': 0.754, 'pos': 0.061, 'compound': -0.9847},\n",
       " {'neg': 0.121, 'neu': 0.779, 'pos': 0.1, 'compound': -0.816},\n",
       " {'neg': 0.297, 'neu': 0.667, 'pos': 0.036, 'compound': -0.996},\n",
       " {'neg': 0.203, 'neu': 0.729, 'pos': 0.068, 'compound': -0.9984},\n",
       " {'neg': 0.178, 'neu': 0.752, 'pos': 0.07, 'compound': -0.9981},\n",
       " {'neg': 0.253, 'neu': 0.675, 'pos': 0.072, 'compound': -0.9928},\n",
       " {'neg': 0.271, 'neu': 0.691, 'pos': 0.038, 'compound': -0.9924},\n",
       " {'neg': 0.15, 'neu': 0.786, 'pos': 0.065, 'compound': -0.9777},\n",
       " {'neg': 0.211, 'neu': 0.7, 'pos': 0.088, 'compound': -0.9944},\n",
       " {'neg': 0.102, 'neu': 0.832, 'pos': 0.066, 'compound': -0.968},\n",
       " {'neg': 0.105, 'neu': 0.772, 'pos': 0.123, 'compound': 0.7425},\n",
       " {'neg': 0.009, 'neu': 0.865, 'pos': 0.126, 'compound': 0.9963},\n",
       " {'neg': 0.016, 'neu': 0.867, 'pos': 0.117, 'compound': 0.9918},\n",
       " {'neg': 0.037, 'neu': 0.936, 'pos': 0.027, 'compound': -0.3549},\n",
       " {'neg': 0.144, 'neu': 0.755, 'pos': 0.101, 'compound': -0.9623},\n",
       " {'neg': 0.0, 'neu': 0.88, 'pos': 0.12, 'compound': 0.7717},\n",
       " {'neg': 0.0, 'neu': 0.868, 'pos': 0.132, 'compound': 0.8519},\n",
       " {'neg': 0.033, 'neu': 0.738, 'pos': 0.229, 'compound': 0.9811},\n",
       " {'neg': 0.079, 'neu': 0.852, 'pos': 0.069, 'compound': -0.3612},\n",
       " {'neg': 0.0, 'neu': 0.965, 'pos': 0.035, 'compound': 0.4767},\n",
       " {'neg': 0.239, 'neu': 0.71, 'pos': 0.051, 'compound': -0.9723},\n",
       " {'neg': 0.155, 'neu': 0.764, 'pos': 0.081, 'compound': -0.9801},\n",
       " {'neg': 0.157, 'neu': 0.743, 'pos': 0.1, 'compound': -0.9711},\n",
       " {'neg': 0.21, 'neu': 0.723, 'pos': 0.066, 'compound': -0.9947},\n",
       " {'neg': 0.026, 'neu': 0.855, 'pos': 0.119, 'compound': 0.9795},\n",
       " {'neg': 0.146, 'neu': 0.786, 'pos': 0.068, 'compound': -0.9744},\n",
       " {'neg': 0.128, 'neu': 0.81, 'pos': 0.062, 'compound': -0.9403},\n",
       " {'neg': 0.115, 'neu': 0.757, 'pos': 0.127, 'compound': 0.477},\n",
       " {'neg': 0.248, 'neu': 0.667, 'pos': 0.085, 'compound': -0.9969},\n",
       " {'neg': 0.129, 'neu': 0.813, 'pos': 0.058, 'compound': -0.9709},\n",
       " {'neg': 0.165, 'neu': 0.728, 'pos': 0.107, 'compound': -0.9781},\n",
       " {'neg': 0.189, 'neu': 0.723, 'pos': 0.087, 'compound': -0.9814},\n",
       " {'neg': 0.285, 'neu': 0.652, 'pos': 0.063, 'compound': -0.9989},\n",
       " {'neg': 0.238, 'neu': 0.666, 'pos': 0.095, 'compound': -0.9971},\n",
       " {'neg': 0.229, 'neu': 0.713, 'pos': 0.058, 'compound': -0.9924},\n",
       " {'neg': 0.194, 'neu': 0.753, 'pos': 0.053, 'compound': -0.997},\n",
       " {'neg': 0.137, 'neu': 0.812, 'pos': 0.051, 'compound': -0.9884},\n",
       " {'neg': 0.135, 'neu': 0.812, 'pos': 0.052, 'compound': -0.9871},\n",
       " {'neg': 0.203, 'neu': 0.703, 'pos': 0.094, 'compound': -0.9828},\n",
       " {'neg': 0.198, 'neu': 0.753, 'pos': 0.049, 'compound': -0.9955},\n",
       " {'neg': 0.189, 'neu': 0.706, 'pos': 0.105, 'compound': -0.9909},\n",
       " {'neg': 0.137, 'neu': 0.762, 'pos': 0.101, 'compound': -0.9407},\n",
       " {'neg': 0.157, 'neu': 0.799, 'pos': 0.044, 'compound': -0.9747},\n",
       " {'neg': 0.202, 'neu': 0.72, 'pos': 0.078, 'compound': -0.9919},\n",
       " {'neg': 0.146, 'neu': 0.762, 'pos': 0.092, 'compound': -0.9609},\n",
       " {'neg': 0.202, 'neu': 0.755, 'pos': 0.043, 'compound': -0.9962},\n",
       " {'neg': 0.127, 'neu': 0.806, 'pos': 0.068, 'compound': -0.9426},\n",
       " {'neg': 0.2, 'neu': 0.768, 'pos': 0.031, 'compound': -0.9946},\n",
       " {'neg': 0.15, 'neu': 0.731, 'pos': 0.119, 'compound': -0.8826},\n",
       " {'neg': 0.123, 'neu': 0.747, 'pos': 0.13, 'compound': 0.8875},\n",
       " {'neg': 0.263, 'neu': 0.693, 'pos': 0.044, 'compound': -0.9981},\n",
       " {'neg': 0.18, 'neu': 0.776, 'pos': 0.044, 'compound': -0.9652},\n",
       " {'neg': 0.168, 'neu': 0.742, 'pos': 0.089, 'compound': -0.9979},\n",
       " {'neg': 0.2, 'neu': 0.738, 'pos': 0.063, 'compound': -0.9992},\n",
       " {'neg': 0.176, 'neu': 0.738, 'pos': 0.087, 'compound': -0.9983},\n",
       " {'neg': 0.234, 'neu': 0.75, 'pos': 0.016, 'compound': -0.9961},\n",
       " {'neg': 0.167, 'neu': 0.765, 'pos': 0.068, 'compound': -0.9531},\n",
       " {'neg': 0.158, 'neu': 0.797, 'pos': 0.045, 'compound': -0.9783},\n",
       " {'neg': 0.172, 'neu': 0.738, 'pos': 0.089, 'compound': -0.9968},\n",
       " {'neg': 0.175, 'neu': 0.734, 'pos': 0.091, 'compound': -0.9885},\n",
       " {'neg': 0.149, 'neu': 0.789, 'pos': 0.063, 'compound': -0.9779},\n",
       " {'neg': 0.197, 'neu': 0.752, 'pos': 0.051, 'compound': -0.9973},\n",
       " {'neg': 0.124, 'neu': 0.846, 'pos': 0.029, 'compound': -0.9932},\n",
       " {'neg': 0.157, 'neu': 0.789, 'pos': 0.054, 'compound': -0.9758},\n",
       " {'neg': 0.189, 'neu': 0.748, 'pos': 0.063, 'compound': -0.9891},\n",
       " {'neg': 0.112, 'neu': 0.861, 'pos': 0.027, 'compound': -0.93},\n",
       " {'neg': 0.153, 'neu': 0.824, 'pos': 0.023, 'compound': -0.9939},\n",
       " {'neg': 0.173, 'neu': 0.752, 'pos': 0.075, 'compound': -0.9837},\n",
       " {'neg': 0.209, 'neu': 0.703, 'pos': 0.087, 'compound': -0.9885},\n",
       " {'neg': 0.218, 'neu': 0.737, 'pos': 0.045, 'compound': -0.9958},\n",
       " {'neg': 0.305, 'neu': 0.661, 'pos': 0.034, 'compound': -0.9964},\n",
       " {'neg': 0.193, 'neu': 0.76, 'pos': 0.047, 'compound': -0.9652},\n",
       " {'neg': 0.135, 'neu': 0.808, 'pos': 0.057, 'compound': -0.994},\n",
       " {'neg': 0.112, 'neu': 0.817, 'pos': 0.071, 'compound': -0.9557},\n",
       " {'neg': 0.199, 'neu': 0.729, 'pos': 0.071, 'compound': -0.9899},\n",
       " {'neg': 0.263, 'neu': 0.695, 'pos': 0.043, 'compound': -0.9988},\n",
       " {'neg': 0.149, 'neu': 0.749, 'pos': 0.102, 'compound': -0.9918},\n",
       " {'neg': 0.122, 'neu': 0.792, 'pos': 0.086, 'compound': -0.9538},\n",
       " {'neg': 0.123, 'neu': 0.783, 'pos': 0.094, 'compound': -0.9213},\n",
       " {'neg': 0.241, 'neu': 0.698, 'pos': 0.061, 'compound': -0.9989},\n",
       " {'neg': 0.186, 'neu': 0.678, 'pos': 0.135, 'compound': -0.9931},\n",
       " {'neg': 0.111, 'neu': 0.863, 'pos': 0.026, 'compound': -0.9623},\n",
       " {'neg': 0.197, 'neu': 0.755, 'pos': 0.048, 'compound': -0.9902},\n",
       " {'neg': 0.203, 'neu': 0.725, 'pos': 0.072, 'compound': -0.9945},\n",
       " {'neg': 0.177, 'neu': 0.772, 'pos': 0.051, 'compound': -0.9988},\n",
       " {'neg': 0.133, 'neu': 0.826, 'pos': 0.041, 'compound': -0.9944},\n",
       " {'neg': 0.189, 'neu': 0.685, 'pos': 0.126, 'compound': -0.9195},\n",
       " {'neg': 0.228, 'neu': 0.711, 'pos': 0.061, 'compound': -0.9945},\n",
       " {'neg': 0.281, 'neu': 0.665, 'pos': 0.054, 'compound': -0.9963},\n",
       " {'neg': 0.274, 'neu': 0.665, 'pos': 0.062, 'compound': -0.99},\n",
       " {'neg': 0.226, 'neu': 0.715, 'pos': 0.059, 'compound': -0.9937},\n",
       " {'neg': 0.256, 'neu': 0.678, 'pos': 0.067, 'compound': -0.9962},\n",
       " {'neg': 0.173, 'neu': 0.738, 'pos': 0.09, 'compound': -0.9915},\n",
       " {'neg': 0.248, 'neu': 0.714, 'pos': 0.038, 'compound': -0.9904},\n",
       " {'neg': 0.157, 'neu': 0.843, 'pos': 0.0, 'compound': -0.9796},\n",
       " {'neg': 0.137, 'neu': 0.754, 'pos': 0.109, 'compound': -0.8894},\n",
       " {'neg': 0.135, 'neu': 0.78, 'pos': 0.085, 'compound': -0.9654},\n",
       " {'neg': 0.154, 'neu': 0.747, 'pos': 0.099, 'compound': -0.9786},\n",
       " {'neg': 0.183, 'neu': 0.717, 'pos': 0.1, 'compound': -0.995},\n",
       " {'neg': 0.181, 'neu': 0.743, 'pos': 0.077, 'compound': -0.9905},\n",
       " {'neg': 0.213, 'neu': 0.706, 'pos': 0.082, 'compound': -0.9983},\n",
       " {'neg': 0.204, 'neu': 0.74, 'pos': 0.055, 'compound': -0.999},\n",
       " {'neg': 0.239, 'neu': 0.703, 'pos': 0.058, 'compound': -0.999},\n",
       " {'neg': 0.222, 'neu': 0.711, 'pos': 0.067, 'compound': -0.9989},\n",
       " {'neg': 0.231, 'neu': 0.751, 'pos': 0.018, 'compound': -0.9974},\n",
       " {'neg': 0.122, 'neu': 0.782, 'pos': 0.095, 'compound': -0.8878},\n",
       " {'neg': 0.159, 'neu': 0.762, 'pos': 0.079, 'compound': -0.9729},\n",
       " {'neg': 0.216, 'neu': 0.712, 'pos': 0.071, 'compound': -0.9972},\n",
       " {'neg': 0.181, 'neu': 0.74, 'pos': 0.078, 'compound': -0.991},\n",
       " {'neg': 0.25, 'neu': 0.684, 'pos': 0.067, 'compound': -0.9989},\n",
       " {'neg': 0.129, 'neu': 0.81, 'pos': 0.061, 'compound': -0.9753},\n",
       " {'neg': 0.219, 'neu': 0.75, 'pos': 0.031, 'compound': -0.9981},\n",
       " {'neg': 0.271, 'neu': 0.696, 'pos': 0.033, 'compound': -0.9983},\n",
       " {'neg': 0.218, 'neu': 0.743, 'pos': 0.039, 'compound': -0.9965},\n",
       " {'neg': 0.207, 'neu': 0.751, 'pos': 0.042, 'compound': -0.9918},\n",
       " {'neg': 0.244, 'neu': 0.71, 'pos': 0.046, 'compound': -0.9949},\n",
       " {'neg': 0.056, 'neu': 0.873, 'pos': 0.072, 'compound': 0.5719},\n",
       " {'neg': 0.072, 'neu': 0.822, 'pos': 0.106, 'compound': 0.8344},\n",
       " {'neg': 0.059, 'neu': 0.846, 'pos': 0.095, 'compound': 0.9459},\n",
       " {'neg': 0.191, 'neu': 0.738, 'pos': 0.07, 'compound': -0.9989},\n",
       " {'neg': 0.172, 'neu': 0.768, 'pos': 0.06, 'compound': -0.9836},\n",
       " {'neg': 0.015, 'neu': 0.847, 'pos': 0.138, 'compound': 0.9982},\n",
       " {'neg': 0.026, 'neu': 0.834, 'pos': 0.14, 'compound': 0.9919},\n",
       " {'neg': 0.082, 'neu': 0.821, 'pos': 0.097, 'compound': 0.0422},\n",
       " {'neg': 0.09, 'neu': 0.826, 'pos': 0.084, 'compound': -0.73},\n",
       " {'neg': 0.242, 'neu': 0.698, 'pos': 0.06, 'compound': -0.9994},\n",
       " {'neg': 0.169, 'neu': 0.807, 'pos': 0.024, 'compound': -0.9801},\n",
       " {'neg': 0.218, 'neu': 0.736, 'pos': 0.046, 'compound': -0.9877},\n",
       " {'neg': 0.14, 'neu': 0.825, 'pos': 0.035, 'compound': -0.9845},\n",
       " {'neg': 0.187, 'neu': 0.727, 'pos': 0.086, 'compound': -0.9837},\n",
       " {'neg': 0.249, 'neu': 0.703, 'pos': 0.048, 'compound': -0.9957},\n",
       " {'neg': 0.228, 'neu': 0.723, 'pos': 0.048, 'compound': -0.9956},\n",
       " {'neg': 0.156, 'neu': 0.764, 'pos': 0.08, 'compound': -0.9773},\n",
       " {'neg': 0.082, 'neu': 0.814, 'pos': 0.104, 'compound': 0.4522},\n",
       " {'neg': 0.105, 'neu': 0.833, 'pos': 0.062, 'compound': -0.8731},\n",
       " {'neg': 0.096, 'neu': 0.802, 'pos': 0.102, 'compound': -0.3041},\n",
       " {'neg': 0.147, 'neu': 0.782, 'pos': 0.071, 'compound': -0.9329},\n",
       " {'neg': 0.093, 'neu': 0.812, 'pos': 0.095, 'compound': -0.1372},\n",
       " {'neg': 0.076, 'neu': 0.843, 'pos': 0.08, 'compound': 0.6771},\n",
       " {'neg': 0.103, 'neu': 0.804, 'pos': 0.093, 'compound': -0.4091},\n",
       " {'neg': 0.127, 'neu': 0.786, 'pos': 0.087, 'compound': -0.9599},\n",
       " {'neg': 0.124, 'neu': 0.793, 'pos': 0.083, 'compound': -0.8332},\n",
       " {'neg': 0.113, 'neu': 0.782, 'pos': 0.105, 'compound': -0.235},\n",
       " {'neg': 0.09, 'neu': 0.84, 'pos': 0.071, 'compound': -0.8288},\n",
       " {'neg': 0.064, 'neu': 0.87, 'pos': 0.066, 'compound': -0.1372},\n",
       " {'neg': 0.071, 'neu': 0.867, 'pos': 0.062, 'compound': -0.3041},\n",
       " {'neg': 0.23, 'neu': 0.691, 'pos': 0.079, 'compound': -0.9919},\n",
       " {'neg': 0.235, 'neu': 0.716, 'pos': 0.049, 'compound': -0.9883},\n",
       " {'neg': 0.161, 'neu': 0.757, 'pos': 0.082, 'compound': -0.9933},\n",
       " {'neg': 0.191, 'neu': 0.769, 'pos': 0.041, 'compound': -0.9938},\n",
       " {'neg': 0.155, 'neu': 0.777, 'pos': 0.067, 'compound': -0.9664},\n",
       " {'neg': 0.085, 'neu': 0.814, 'pos': 0.101, 'compound': 0.5667},\n",
       " {'neg': 0.171, 'neu': 0.741, 'pos': 0.088, 'compound': -0.9696},\n",
       " {'neg': 0.141, 'neu': 0.758, 'pos': 0.101, 'compound': -0.9789},\n",
       " {'neg': 0.161, 'neu': 0.807, 'pos': 0.033, 'compound': -0.9925}]"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the function to each press release in doj_subset\n",
    "sentiment_scores_list = [process_press_release(press_release) for press_release in doj_subset['contents']]\n",
    "sentiment_scores_list\n",
    "# Print the sentiment scores\n",
    "#for i, scores in enumerate(sentiment_scores_list):\n",
    "   # print(f\"Press Release {i+1} Sentiment Scores:\", scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C. Add the four sentiment scores to the `doj_subset` dataframe to create a dataframe: `doj_subset_wscore`. Sort from highest neg to lowest neg score and print the top `id`, `contents`, and `neg` columns of the two most neg press releases. \n",
    "\n",
    "Notes:\n",
    "\n",
    "- Don't worry if your sentiment score differs slightly from our output on GitHub; differences in preprocessing can lead to diff scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 2 Most Negative Press Releases:\n",
      "           id  \\\n",
      "329    14-248   \n",
      "11593  16-718   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               contents  \\\n",
      "329                                                                                                                                                                                                                                                                                                                                                                                                                                                                         The Department of Justice announced that this morning John W. Ng, 58, of Albuquerque, N.M., made his initial appearance in federal court on a criminal complaint charging him with a hate crime offense.  This charge is related to anti-Semitic threats Ng made against a Jewish woman who owns and operates the Nosh Jewish Delicatessen and Bakery in Albuquerque. Ng was arrested by the FBI on March 7, 2014, based on a criminal complaint alleging that he interfered with the victim’s federally protected rights by threatening her and interfering with her business because of her religion.  According to the criminal complaint, between Jan. 22, 2014, and Feb. 8, 2014, Ng allegedly posted threatening anti-Semitic notes on and in the vicinity of the victim’s business. A criminal complaint merely establishes probable cause, and Ng is presumed innocent unless proven guilty.  If convicted on the offense charged in the criminal complaint, Ng faces a maximum statutory penalty of one year in prison. This matter was investigated by the Albuquerque Division of the FBI and is being prosecuted by Assistant U.S. Attorney Mark T. Baker of the U.S. Attorney’s Office for the District of New Mexico and Trial Attorney AeJean Cha of the U.S. Department of Justice’s Civil Rights Division.   \n",
      "11593  In a nine-count indictment unsealed today, two Mississippi correctional officers were charged with beating an inmate and a third was charged with helping to cover it up.  The indictment charged Lawardrick Marsher, 28, and Robert Sturdivant, 47, officers at Mississippi State Penitentiary, in Parchman, Mississippi, with a beating that included kicking, punching and throwing the victim to the ground.  Marsher and Sturdivant were charged with violating the right of K.H., a convicted prisoner, to be free from cruel and unusual punishment.  Sturdivant was also charged with failing to intervene while Marsher was punching and beating K.H.  The indictment alleges that their actions involved the use of a dangerous weapon and resulted in bodily injury to the victim. A third officer, Deonte Pate, 23, was charged along with Marsher and Sturdivant for conspiring to cover up the beating.  The indictment alleges that all three officers submitted false reports and that all three lied to the FBI. If convicted, Marsher and Sturdivant face a maximum sentence of 10 years in prison on the excessive force charges.  Each of the three officers faces up to five years in prison on the conspiracy and false statement charges, and up to 20 years in prison on the false report charges. An indictment is merely an accusation, and the defendants are presumed innocent unless and until proven guilty. This case is being investigated by the FBI’s Jackson Division, with the cooperation of the Mississippi Department of Corrections.  It is being prosecuted by Assistant U.S. Attorney Robert Coleman of the Northern District of Mississippi and Trial Attorney Dana Mulhauser of the Civil Rights Division’s Criminal Section.     Marsher Indictment   \n",
      "\n",
      "         neg  \n",
      "329    0.341  \n",
      "11593  0.305  \n"
     ]
    }
   ],
   "source": [
    "# Apply the process_press_release function to each press release in doj_subset\n",
    "sentiment_scores_list = [process_press_release(press_release) for press_release in doj_subset['contents']]\n",
    "\n",
    "# Create a DataFrame with sentiment scores\n",
    "doj_subset_wscore = doj_subset.copy()\n",
    "doj_subset_wscore['neg'] = [scores['neg'] for scores in sentiment_scores_list]\n",
    "doj_subset_wscore['neu'] = [scores['neu'] for scores in sentiment_scores_list]\n",
    "doj_subset_wscore['pos'] = [scores['pos'] for scores in sentiment_scores_list]\n",
    "doj_subset_wscore['compound'] = [scores['compound'] for scores in sentiment_scores_list]\n",
    "\n",
    "# Sort doj_subset_wscore from highest neg to lowest neg score\n",
    "doj_subset_wscore = doj_subset_wscore.sort_values(by='neg', ascending=False)\n",
    "\n",
    "# Print the top id, contents, and neg columns of the two most negative press releases\n",
    "print(\"Top 2 Most Negative Press Releases:\")\n",
    "print(doj_subset_wscore[['id', 'contents', 'neg']].head(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D. With the dataframe from part C, find the mean compound sentiment score for each of the three topics in `topics_clean` using group_by and agg.\n",
    "\n",
    "E. Add a 1 sentence interpretation of why we might see the variation in scores (remember that compound is a standardized summary where -1 is most negative; +1 is most positive)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Compound Sentiment Score for Each Topic:\n",
      "topics_clean\n",
      "Civil Rights             -0.109418\n",
      "Hate Crimes              -0.936392\n",
      "Project Safe Childhood   -0.707904\n",
      "Name: compound, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Group by topics_clean and calculate the mean compound sentiment score for each group\n",
    "mean_compound_scores = doj_subset_wscore.groupby('topics_clean')['compound'].agg('mean')\n",
    "\n",
    "# Print the mean compound sentiment score for each topic\n",
    "print(\"Mean Compound Sentiment Score for Each Topic:\")\n",
    "print(mean_compound_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variation in mean compound sentiment scores across different topics could be attributed to the varying nature and intensity of the events and issues associated with each topic. For instance, topics like Hate Crimes might elicit more negative sentiment due to the severity and emotional impact of such crimes, resulting in lower mean compound scores compared to topics like Civil Rights, where sentiments may be more mixed or nuanced. Similarly, Project Safe Childhood, while focusing on child safety, may involve sensitive and distressing subject matter, contributing to a lower mean compound score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Topic modeling (25 points)\n",
    "\n",
    "For this question, use the `doj_subset_wscores` data that is restricted to civil rights, hate crimes, and project safe childhood and with the sentiment scores added\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Preprocess the data by removing stopwords, punctuation, and non-alpha words (5 points)\n",
    "\n",
    "A. Write a function that:\n",
    "\n",
    "- Takes in a single raw string in the `contents` column from that dataframe\n",
    "- Does the following preprocessing steps:\n",
    "\n",
    "    - Converts the words to lowercase\n",
    "    - Removes stopwords, adding the custom stopwords in your code cell below to the default stopwords list\n",
    "    - Only retains alpha words (so removes digits and punctuation)\n",
    "    - Only retains words 4 characters or longer\n",
    "    - Uses the snowball stemmer from nltk to stem\n",
    "\n",
    "- Returns a joined preprocessed string\n",
    "    \n",
    "B. Use `apply` or list comprehension to execute that function and create a new column in the data called `processed_text`\n",
    "    \n",
    "C. Print the `id`, `contents`, and `processed_text` columns for the following press releases:\n",
    "\n",
    "id = 16-718 (this case: https://www.seattletimes.com/nation-world/doj-miami-police-reach-settlement-in-civil-rights-case/)\n",
    "\n",
    "id = 16-217 (this case: https://www.wlbt.com/story/32275512/three-mississippi-correctional-officers-indicted-for-inmate-assault-and-cover-up/)\n",
    "    \n",
    "**Resources**:\n",
    "\n",
    "- Here's code examples for the snowball stemmer: https://www.geeksforgeeks.org/snowball-stemmer-nlp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Snowball stemmer and stopwords\n",
    "snowball_stemmer = SnowballStemmer(language='english')\n",
    "custom_doj_stopwords = [\"civil\", \"rights\", \"division\", \"department\", \"justice\",\n",
    "                        \"office\", \"attorney\", \"district\", \"case\", \"investigation\", \"assistant\",\n",
    "                       \"trial\", \"assistance\", \"assist\"]\n",
    "\n",
    "def preprocess_text(raw_text):\n",
    "    # Convert the text to lowercase\n",
    "    raw_text = raw_text.lower()\n",
    "    \n",
    "    # Tokenize the text into words\n",
    "    words = nltk.word_tokenize(raw_text)\n",
    "    \n",
    "    # Remove stopwords, custom stopwords, punctuation, and non-alpha words, and retain words longer than 3 characters\n",
    "    filtered_words = [word for word in words if word not in set(stopwords.words('english') + list(string.punctuation) + custom_doj_stopwords) and len(word) >= 4 and word.isalpha()]\n",
    "    \n",
    "    # Stem the words using Snowball stemmer\n",
    "    stemmed_words = [snowball_stemmer.stem(word) for word in filtered_words]\n",
    "    \n",
    "    # Join the stemmed words back into a single string\n",
    "    preprocessed_text = ' '.join(stemmed_words)\n",
    "    \n",
    "    return preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the preprocess_text function to each row in the contents column and create a new column processed_text\n",
    "doj_subset_wscore['processed_text'] = doj_subset_wscore['contents'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           id  \\\n",
      "11593  16-718   \n",
      "6727   16-217   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         contents  \\\n",
      "11593                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            In a nine-count indictment unsealed today, two Mississippi correctional officers were charged with beating an inmate and a third was charged with helping to cover it up.  The indictment charged Lawardrick Marsher, 28, and Robert Sturdivant, 47, officers at Mississippi State Penitentiary, in Parchman, Mississippi, with a beating that included kicking, punching and throwing the victim to the ground.  Marsher and Sturdivant were charged with violating the right of K.H., a convicted prisoner, to be free from cruel and unusual punishment.  Sturdivant was also charged with failing to intervene while Marsher was punching and beating K.H.  The indictment alleges that their actions involved the use of a dangerous weapon and resulted in bodily injury to the victim. A third officer, Deonte Pate, 23, was charged along with Marsher and Sturdivant for conspiring to cover up the beating.  The indictment alleges that all three officers submitted false reports and that all three lied to the FBI. If convicted, Marsher and Sturdivant face a maximum sentence of 10 years in prison on the excessive force charges.  Each of the three officers faces up to five years in prison on the conspiracy and false statement charges, and up to 20 years in prison on the false report charges. An indictment is merely an accusation, and the defendants are presumed innocent unless and until proven guilty. This case is being investigated by the FBI’s Jackson Division, with the cooperation of the Mississippi Department of Corrections.  It is being prosecuted by Assistant U.S. Attorney Robert Coleman of the Northern District of Mississippi and Trial Attorney Dana Mulhauser of the Civil Rights Division’s Criminal Section.     Marsher Indictment   \n",
      "6727   The Justice Department has reached a comprehensive settlement agreement with the city of Miami and the Miami Police Department (MPD) resolving the Justice Department’s investigation of officer-involved shootings by MPD officers, announced Principal Deputy Assistant Attorney General Vanita Gupta, head of the Justice Department’s Civil Rights Division and U.S. Attorney Wifredo A. Ferrer of the Southern District of Florida. The settlement, which was approved by Miami’s city commission today and will go into effect when the agreement is signed by all parties, resolves claims stemming from the Justice Department’s investigation into officer-involved shootings by MPD officers, which was conducted under the Violent Crime Control and Law Enforcement Act of 1994.  The investigation’s findings, issued in July 2013, identified a pattern or practice of excessive use of force through officer-involved shootings in violation of the Fourth Amendment of the Constitution.  The city’s compliance with the settlement will be monitored by an independent reviewer, former Tampa, Florida, Police Chief Jane Castor.  Under the settlement agreement, the city will implement comprehensive reforms to ensure constitutional policing and support public trust.  The settlement agreement is designed to minimize officer-involved shootings and to more effectively and quickly investigate officer-involved shootings that do occur, through measures that include: “This settlement represents a renewed commitment by the city of Miami and Chief Rodolfo Llanes to provide constitutional policing for Miami residents and to protect public safety through sustainable reform,” said Principal Deputy Assistant Attorney General Gupta.  “The agreement will help to strengthen the relationship between the MPD and the communities they serve by improving accountability for officers who fire their weapons unlawfully, and provides for community participation in the enforcement of this agreement.”  “Today's agreement is the result of a joint effort between the Department of Justice and the City of Miami to ensure that the Miami Police Department continues its efforts to make our community safe while protecting the sacred Constitutional rights of all of our citizens,” said U.S. Attorney Ferrer.  “Through oversight and communication, the agreement seeks to make permanent the positive changes that former Chief Orosa and Chief Llanes have made, and we applaud the City Commission’s vote.” The settlement agreement builds upon important reforms implemented by the city since the Justice Department issued its findings, including:  The investigation was conducted by attorneys and staff from the Civil Rights Division’s Special Litigation Section and the Civil Division of the U. S. Attorney’s Office of the Southern District of Florida.   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            processed_text  \n",
      "11593                                                                                                                                                                                                                                                                                                                                                                                                                                                                   indict unseal today mississippi correct offic charg beat inmat third charg help cover indict charg lawardrick marsher robert sturdiv offic mississippi state penitentiari parchman mississippi beat includ kick punch throw victim ground marsher sturdiv charg violat right convict prison free cruel unusu punish sturdiv also charg fail interven marsher punch beat indict alleg action involv danger weapon result bodili injuri victim third offic deont pate charg along marsher sturdiv conspir cover beat indict alleg three offic submit fals report three lie convict marsher sturdiv face maximum sentenc year prison excess forc charg three offic face five year prison conspiraci fals statement charg year prison fals report charg indict mere accus defend presum innoc unless proven guilti investig jackson cooper mississippi correct prosecut robert coleman northern mississippi dana mulhaus crimin section marsher indict  \n",
      "6727   reach comprehens settlement agreement citi miami miami polic resolv shoot offic announc princip deputi general vanita gupta head wifredo ferrer southern florida settlement approv miami citi commiss today effect agreement sign parti resolv claim stem shoot offic conduct violent crime control enforc find issu juli identifi pattern practic excess forc shoot violat fourth amend constitut citi complianc settlement monitor independ review former tampa florida polic chief jane castor settlement agreement citi implement comprehens reform ensur constitut polic support public trust settlement agreement design minim shoot effect quick investig shoot occur measur includ settlement repres renew commit citi miami chief rodolfo llane provid constitut polic miami resid protect public safeti sustain reform said princip deputi general gupta agreement help strengthen relationship communiti serv improv account offic fire weapon unlaw provid communiti particip enforc today agreement result joint effort citi miami ensur miami polic continu effort make communiti safe protect sacr constitut citizen said ferrer oversight communic agreement seek make perman posit chang former chief orosa chief llane made applaud citi commiss settlement agreement build upon import reform implement citi sinc issu find includ conduct attorney staff special litig section southern florida  \n"
     ]
    }
   ],
   "source": [
    "# Filter the DataFrame for the specified press releases\n",
    "selected_press_releases = doj_subset_wscore[doj_subset_wscore['id'].isin(['16-718', '16-217'])]\n",
    "\n",
    "# Print the id, contents, and processed_text columns for the selected press releases\n",
    "print(selected_press_releases[['id', 'contents', 'processed_text']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Create a document-term matrix from the preprocessed press releases and to explore top words (5 points)\n",
    "\n",
    "A. Use the `create_dtm` function I provide (alternately, feel free to write your own!) and create a document-term matrix using the preprocessed press releases; make sure metadata contains the following columns: `id`, `compound` sentiment column you added, and the `topics_clean` column\n",
    "\n",
    "B. Print the top 10 words for press releases with compound sentiment in the top 5% (so the most positive sentiment)\n",
    "\n",
    "C. Print the top 10 words for press releases with compound sentiment in the bottom 5% (so the most negative sentiment)\n",
    "\n",
    "**Hint**: for these, remember the pandas quantile function from pset one.  \n",
    "\n",
    "D. Print the top 10 words for press releases in each of the three `topics_clean`\n",
    "\n",
    "For steps B - D, to receive full credit, write a function `get_topwords` that helps you avoid duplicated code when you find top words for the different subsets of the data. There are different ways to structure it but one way is to feed it subsetted data (so data subsetted to one topic etc.) and for it to get the top words for that subset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dtm(list_of_strings, metadata):\n",
    "    vectorizer = CountVectorizer(lowercase=True)\n",
    "    dtm_sparse = vectorizer.fit_transform(list_of_strings)\n",
    "    dtm_dense_named = pd.DataFrame(dtm_sparse.todense(), columns=vectorizer.get_feature_names_out())\n",
    "    dtm_dense_named_withid = pd.concat([metadata.reset_index(), dtm_dense_named], axis=1)\n",
    "    return dtm_dense_named_withid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the document-term matrix using the provided create_dtm function\n",
    "dtm = create_dtm(doj_subset_wscore['processed_text'], doj_subset_wscore[['id', 'compound', 'topics_clean']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topwords(subset_data):\n",
    "    # Concatenate all preprocessed text\n",
    "    all_text = ' '.join(subset_data['processed_text'])\n",
    "\n",
    "    # Create a document-term matrix for the concatenated text\n",
    "    dtm_subset = create_dtm([all_text], pd.DataFrame())\n",
    "\n",
    "    # Get the top 10 words\n",
    "    top_10_words = dtm_subset.iloc[0, 3:].sort_values(ascending=False)[:10]\n",
    "    \n",
    "    return top_10_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words for press releases with compound sentiment in the top 5%:\n",
      "agreement     173.0\n",
      "enforc        133.0\n",
      "state         118.0\n",
      "disabl        110.0\n",
      "ensur         108.0\n",
      "communiti      94.0\n",
      "polic          93.0\n",
      "settlement     86.0\n",
      "general        86.0\n",
      "student        82.0\n",
      "Name: 0, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Step B: Top 10 words for press releases with compound sentiment in the top 5%\n",
    "top_5_percent = doj_subset_wscore[doj_subset_wscore['compound'] >= threshold]\n",
    "topwords_top_5_percent = get_topwords(top_5_percent)\n",
    "print(\"Top 10 words for press releases with compound sentiment in the top 5%:\")\n",
    "print(topwords_top_5_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words for press releases with compound sentiment in the top 5%:\n",
      "agreement     173.0\n",
      "enforc        133.0\n",
      "state         118.0\n",
      "disabl        110.0\n",
      "ensur         108.0\n",
      "communiti      94.0\n",
      "polic          93.0\n",
      "settlement     86.0\n",
      "general        86.0\n",
      "student        82.0\n",
      "Name: 0, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Part B\n",
    "# Calculate the threshold for the top 5% compound sentiment score\n",
    "threshold = doj_subset_wscore['compound'].quantile(0.95)\n",
    "\n",
    "# Filter press releases with compound sentiment in the top 5%\n",
    "top_5_percent = doj_subset_wscore[doj_subset_wscore['compound'] >= threshold]\n",
    "\n",
    "# Concatenate all preprocessed text from press releases in the top 5%\n",
    "top_text = ' '.join(top_5_percent['processed_text'])\n",
    "\n",
    "# Create a document-term matrix for the press releases in the top 5%\n",
    "dtm_top_5_percent = create_dtm([top_text], pd.DataFrame())\n",
    "\n",
    "# Get the top 10 words\n",
    "top_10_words = dtm_top_5_percent.iloc[0, 3:].sort_values(ascending=False)[:10]\n",
    "\n",
    "# Print the top 10 words\n",
    "print(\"Top 10 words for press releases with compound sentiment in the top 5%:\")\n",
    "print(top_10_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 words for press releases with compound sentiment in the bottom 5%:\n",
      "assault     198.0\n",
      "victim      172.0\n",
      "crime       160.0\n",
      "hate        127.0\n",
      "offic       120.0\n",
      "defend      117.0\n",
      "sentenc     111.0\n",
      "charg       101.0\n",
      "anderson     93.0\n",
      "guilti       92.0\n",
      "Name: 0, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Step C: Top 10 words for press releases with compound sentiment in the bottom 5%\n",
    "bottom_5_percent = doj_subset_wscore[doj_subset_wscore['compound'] <= threshold_bottom]\n",
    "topwords_bottom_5_percent = get_topwords(bottom_5_percent)\n",
    "print(\"\\nTop 10 words for press releases with compound sentiment in the bottom 5%:\")\n",
    "print(topwords_bottom_5_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words for press releases with compound sentiment in the bottom 5%:\n",
      "assault     198.0\n",
      "victim      172.0\n",
      "crime       160.0\n",
      "hate        127.0\n",
      "offic       120.0\n",
      "defend      117.0\n",
      "sentenc     111.0\n",
      "charg       101.0\n",
      "anderson     93.0\n",
      "guilti       92.0\n",
      "Name: 0, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Part C\n",
    "# Calculate the threshold for the bottom 5% compound sentiment score\n",
    "threshold_bottom = doj_subset_wscore['compound'].quantile(0.05)\n",
    "\n",
    "# Filter press releases with compound sentiment in the bottom 5%\n",
    "bottom_5_percent = doj_subset_wscore[doj_subset_wscore['compound'] <= threshold_bottom]\n",
    "\n",
    "# Concatenate all preprocessed text from press releases in the bottom 5%\n",
    "bottom_text = ' '.join(bottom_5_percent['processed_text'])\n",
    "\n",
    "# Create a document-term matrix for the concatenated text\n",
    "dtm_bottom_5_percent = create_dtm([bottom_text], pd.DataFrame())\n",
    "\n",
    "# Get the top 10 words\n",
    "top_10_words_bottom = dtm_bottom_5_percent.iloc[0, 3:].sort_values(ascending=False)[:10]\n",
    "\n",
    "# Print the top 10 words\n",
    "print(\"Top 10 words for press releases with compound sentiment in the bottom 5%:\")\n",
    "print(top_10_words_bottom)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 words for press releases in 'Civil Rights':\n",
      "offic        627.0\n",
      "hous         620.0\n",
      "discrimin    541.0\n",
      "enforc       531.0\n",
      "disabl       509.0\n",
      "said         497.0\n",
      "feder        475.0\n",
      "violat       470.0\n",
      "state        443.0\n",
      "general      408.0\n",
      "Name: 0, dtype: float64\n",
      "\n",
      "Top 10 words for press releases in 'Hate Crimes':\n",
      "victim      590.0\n",
      "crime       533.0\n",
      "prosecut    476.0\n",
      "hate        472.0\n",
      "defend      459.0\n",
      "sentenc     455.0\n",
      "charg       452.0\n",
      "guilti      430.0\n",
      "feder       426.0\n",
      "said        424.0\n",
      "Name: 0, dtype: float64\n",
      "\n",
      "Top 10 words for press releases in 'Project Safe Childhood':\n",
      "child          1018.0\n",
      "exploit         698.0\n",
      "sexual          570.0\n",
      "safe            476.0\n",
      "project         472.0\n",
      "childhood       472.0\n",
      "pornographi     447.0\n",
      "children        416.0\n",
      "crimin          404.0\n",
      "prosecut        374.0\n",
      "Name: 0, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Step D: Top 10 words for press releases in each of the three topics_clean\n",
    "for topic, subset_data in doj_subset_wscore.groupby('topics_clean'):\n",
    "    print(f\"\\nTop 10 words for press releases in '{topic}':\")\n",
    "    topwords_topic = get_topwords(subset_data)\n",
    "    print(topwords_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words for press releases in the topic 'Civil Rights':\n",
      "offic        627.0\n",
      "hous         620.0\n",
      "discrimin    541.0\n",
      "enforc       531.0\n",
      "disabl       509.0\n",
      "said         497.0\n",
      "feder        475.0\n",
      "violat       470.0\n",
      "state        443.0\n",
      "general      408.0\n",
      "Name: 0, dtype: float64\n",
      "\n",
      "\n",
      "Top 10 words for press releases in the topic 'Hate Crimes':\n",
      "victim      590.0\n",
      "crime       533.0\n",
      "prosecut    476.0\n",
      "hate        472.0\n",
      "defend      459.0\n",
      "sentenc     455.0\n",
      "charg       452.0\n",
      "guilti      430.0\n",
      "feder       426.0\n",
      "said        424.0\n",
      "Name: 0, dtype: float64\n",
      "\n",
      "\n",
      "Top 10 words for press releases in the topic 'Project Safe Childhood':\n",
      "child          1018.0\n",
      "exploit         698.0\n",
      "sexual          570.0\n",
      "safe            476.0\n",
      "project         472.0\n",
      "childhood       472.0\n",
      "pornographi     447.0\n",
      "children        416.0\n",
      "crimin          404.0\n",
      "prosecut        374.0\n",
      "Name: 0, dtype: float64\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Part D\n",
    "def get_top_words(subset_data):\n",
    "    # Concatenate all preprocessed text from the subset\n",
    "    subset_text = ' '.join(subset_data['processed_text'])\n",
    "    \n",
    "    # Create a document-term matrix for the concatenated text\n",
    "    dtm_subset = create_dtm([subset_text], pd.DataFrame())\n",
    "    \n",
    "    # Get the top 10 words\n",
    "    top_10_words_subset = dtm_subset.iloc[0, 3:].sort_values(ascending=False)[:10]\n",
    "    \n",
    "    return top_10_words_subset\n",
    "\n",
    "# Define a function to print the top 10 words for a given topic\n",
    "def print_top_10_words_for_topic(topic):\n",
    "    # Filter press releases for the given topic\n",
    "    topic_press_releases = doj_subset_wscore[doj_subset_wscore['topics_clean'] == topic]\n",
    "    \n",
    "    # Get the top words for the topic using the get_top_words function\n",
    "    top_words_topic = get_top_words(topic_press_releases)\n",
    "    \n",
    "    # Print the top 10 words for the topic\n",
    "    print(f\"Top 10 words for press releases in the topic '{topic}':\")\n",
    "    print(top_words_topic)\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Iterate over each topic and print the top 10 words\n",
    "for topic in ['Civil Rights', 'Hate Crimes', 'Project Safe Childhood']:\n",
    "    print_top_10_words_for_topic(topic)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Estimate a topic model using those preprocessed words (5 points)\n",
    "\n",
    "A. Going back to the preprocessed words from part 2.3.1, estimate a topic model with 3 topics, since you want to see if the unsupervised topic models recover different themes for each of the three manually-labeled areas (civil rights; hate crimes; project safe childhood). You have free rein over the other topic model parameters beyond the number of topics.\n",
    "\n",
    "B. After estimating the topic model, print the top 15 words in each topic.\n",
    "\n",
    "**Hints and Resources**:\n",
    "\n",
    "- Same topic modeling resources linked to above\n",
    "- Make sure to use the `random_state` argument within the model so that the numbering of topics does not move around between runs of your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Add topics back to main data and explore correlation between manual labels and our estimated topics (10 points)\n",
    "\n",
    "A. Extract the document-level topic probabilities. Within `get_document_topics`, use the argument `minimum_probability` = 0 to make sure all 3 topic probabilities are returned. Write an assert statement to make sure the length of the list is equal to the number of rows in the `doj_subset_wscores` dataframe\n",
    "\n",
    "B. Add the topic probabilities to the `doj_subset_wscores` dataframe as columns and create a column, `top_topic`, that reflects each document to its highest-probability topic (eg topic 1, 2, or 3)\n",
    "\n",
    "C. For each of the manual labels in `topics_clean` (Hate Crime, Civil Rights, Project Safe Childhood), print the breakdown of the % of documents with each top topic (so, for instance, Hate Crime has 246 documents-- if 123 of those documents are coded to topic_1, that would be 50%; and so on). **Hint**: pd.crosstab and normalize may be helpful: https://pandas.pydata.org/pandas-docs/version/0.23/generated/pandas.crosstab.html\n",
    "\n",
    "D. Using a couple press releases as examples, write a 1-2 sentence interpretation of why some of the manual topics map on more cleanly to an estimated topic than other manual topic(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here to get doc-level topic probabilities \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here to add those topic probabilities to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here to summarize the topic proportions for each of the topics_clean "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Extend the analysis from unigrams to bigrams (10 points)\n",
    "\n",
    "In the previous question, you found top words via a unigram representation of the text. Now, we want to see how those top words change with bigrams (pairs of words)\n",
    "\n",
    "A. Using the `doj_subset_wscore` data and the `processed_text` column (so the words after stemming/other preprocessing), create a column in the data called `processed_text_bigrams` that combines each consecutive pairs of word into a bigram separated by an underscore. Eg:\n",
    "\n",
    "\"depart reach settlem\" would become \"depart_reach reach_settlem\"\n",
    "\n",
    "Do this by writing a function `create_bigram_onedoc` that takes in a single `processed_text` string and returns a string with its bigrams structured similarly to above example\n",
    " \n",
    "**Hint**: there are many ways to solve but `zip` may be helpful: https://stackoverflow.com/questions/21303224/iterate-over-all-pairs-of-consecutive-items-in-a-list\n",
    "\n",
    "B. Print the `id`, `processed_text`, and `processed_text_bigram` columns for press release with id = 16-217"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part A\n",
    "def create_bigram_onedoc(text):\n",
    "    # Split the text into words\n",
    "    words = text.split()\n",
    "    # Use zip to iterate over pairs of consecutive words\n",
    "    bigrams = ['{}_{}'.format(words[i], words[i+1]) for i in range(len(words)-1)]\n",
    "    # Join the bigrams with spaces\n",
    "    return ' '.join(bigrams)\n",
    "\n",
    "doj_subset_wscore['processed_text_bigram'] = doj_subset_wscore['processed_text'].apply(create_bigram_onedoc)\n",
    "\n",
    "\n",
    "# text_example = \"depart reach settlem\"\n",
    "# bigram_example = create_bigram_onedoc(text_example)\n",
    "# print(bigram_example)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: 16-217\n",
      "Processed Text: reach comprehens settlement agreement citi miami miami polic resolv shoot offic announc princip deputi general vanita gupta head wifredo ferrer southern florida settlement approv miami citi commiss today effect agreement sign parti resolv claim stem shoot offic conduct violent crime control enforc find issu juli identifi pattern practic excess forc shoot violat fourth amend constitut citi complianc settlement monitor independ review former tampa florida polic chief jane castor settlement agreement citi implement comprehens reform ensur constitut polic support public trust settlement agreement design minim shoot effect quick investig shoot occur measur includ settlement repres renew commit citi miami chief rodolfo llane provid constitut polic miami resid protect public safeti sustain reform said princip deputi general gupta agreement help strengthen relationship communiti serv improv account offic fire weapon unlaw provid communiti particip enforc today agreement result joint effort citi miami ensur miami polic continu effort make communiti safe protect sacr constitut citizen said ferrer oversight communic agreement seek make perman posit chang former chief orosa chief llane made applaud citi commiss settlement agreement build upon import reform implement citi sinc issu find includ conduct attorney staff special litig section southern florida\n",
      "Processed Text Bigram: reach_comprehens comprehens_settlement settlement_agreement agreement_citi citi_miami miami_miami miami_polic polic_resolv resolv_shoot shoot_offic offic_announc announc_princip princip_deputi deputi_general general_vanita vanita_gupta gupta_head head_wifredo wifredo_ferrer ferrer_southern southern_florida florida_settlement settlement_approv approv_miami miami_citi citi_commiss commiss_today today_effect effect_agreement agreement_sign sign_parti parti_resolv resolv_claim claim_stem stem_shoot shoot_offic offic_conduct conduct_violent violent_crime crime_control control_enforc enforc_find find_issu issu_juli juli_identifi identifi_pattern pattern_practic practic_excess excess_forc forc_shoot shoot_violat violat_fourth fourth_amend amend_constitut constitut_citi citi_complianc complianc_settlement settlement_monitor monitor_independ independ_review review_former former_tampa tampa_florida florida_polic polic_chief chief_jane jane_castor castor_settlement settlement_agreement agreement_citi citi_implement implement_comprehens comprehens_reform reform_ensur ensur_constitut constitut_polic polic_support support_public public_trust trust_settlement settlement_agreement agreement_design design_minim minim_shoot shoot_effect effect_quick quick_investig investig_shoot shoot_occur occur_measur measur_includ includ_settlement settlement_repres repres_renew renew_commit commit_citi citi_miami miami_chief chief_rodolfo rodolfo_llane llane_provid provid_constitut constitut_polic polic_miami miami_resid resid_protect protect_public public_safeti safeti_sustain sustain_reform reform_said said_princip princip_deputi deputi_general general_gupta gupta_agreement agreement_help help_strengthen strengthen_relationship relationship_communiti communiti_serv serv_improv improv_account account_offic offic_fire fire_weapon weapon_unlaw unlaw_provid provid_communiti communiti_particip particip_enforc enforc_today today_agreement agreement_result result_joint joint_effort effort_citi citi_miami miami_ensur ensur_miami miami_polic polic_continu continu_effort effort_make make_communiti communiti_safe safe_protect protect_sacr sacr_constitut constitut_citizen citizen_said said_ferrer ferrer_oversight oversight_communic communic_agreement agreement_seek seek_make make_perman perman_posit posit_chang chang_former former_chief chief_orosa orosa_chief chief_llane llane_made made_applaud applaud_citi citi_commiss commiss_settlement settlement_agreement agreement_build build_upon upon_import import_reform reform_implement implement_citi citi_sinc sinc_issu issu_find find_includ includ_conduct conduct_attorney attorney_staff staff_special special_litig litig_section section_southern southern_florida\n"
     ]
    }
   ],
   "source": [
    "# Part B\n",
    "# Filter the DataFrame to get the row with the specified ID\n",
    "press_release_16_217 = doj_subset_wscore[doj_subset_wscore['id'] == '16-217']\n",
    "\n",
    "# Extract the required columns\n",
    "id_16_217 = press_release_16_217['id'].values[0]\n",
    "processed_text_16_217 = press_release_16_217['processed_text'].values[0]\n",
    "processed_text_bigram_16_217 = press_release_16_217['processed_text_bigram'].values[0]\n",
    "\n",
    "# Print the id, processed_text, and processed_text_bigram columns\n",
    "print(\"ID:\", id_16_217)\n",
    "print(\"Processed Text:\", processed_text_16_217)\n",
    "print(\"Processed Text Bigram:\", processed_text_bigram_16_217)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C. Use the create_dtm function and the `processed_text_bigrams` column to create a document-term matrix (`dtm_bigram`) with these bigrams. Keep the following three columns in the data: `id`, `topics_clean`, and `compound` \n",
    "\n",
    "D. Print the (1) dimensions of the `dtm` matrix from question 2.2  and (2) the dimensions of the `dtm_bigram` matrix. Comment on why the bigram matrix has more dimensions than the unigram matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part C\n",
    "# Create a document-term matrix using the processed_text_bigrams column\n",
    "dtm_bigram = create_dtm(doj_subset_wscore['processed_text_bigram'], doj_subset_wscore[['id', 'topics_clean', 'compound']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of dtm matrix from question 2.2: (717, 6760)\n",
      "Dimensions of dtm_bigram matrix: (717, 71336)\n"
     ]
    }
   ],
   "source": [
    "# Part D\n",
    "# Print the dimensions of the dtm matrix from question 2.2\n",
    "print(\"Dimensions of dtm matrix from question 2.2:\", dtm.shape)\n",
    "\n",
    "# Print the dimensions of the dtm_bigram matrix\n",
    "print(\"Dimensions of dtm_bigram matrix:\", dtm_bigram.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bigram matrix has more dimensions than the unigram matrix because it considers pairs of consecutive words (bigrams) as features, whereas the unigram matrix considers single words (unigrams) as features. Therefore, the bigram matrix captures more detailed information about the relationships between words in the text, leading to a higher number of dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E. Find and print the 10 most prevelant bigrams for each of the three topics_clean using the `get_topwords` function from 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 bigrams for Civil Rights :\n",
      "fair_hous         227.0\n",
      "deputi_general    221.0\n",
      "princip_deputi    221.0\n",
      "vanita_gupta      202.0\n",
      "gupta_head        200.0\n",
      "general_vanita    199.0\n",
      "said_princip      186.0\n",
      "unit_state        152.0\n",
      "nation_origin     141.0\n",
      "consent_decre     127.0\n",
      "dtype: float64\n",
      "\n",
      "Top 10 bigrams for Hate Crimes :\n",
      "hate_crime       368.0\n",
      "plead_guilti     275.0\n",
      "year_prison      160.0\n",
      "special_agent    117.0\n",
      "thoma_perez      111.0\n",
      "grand_juri       101.0\n",
      "perez_general     95.0\n",
      "said_thoma        91.0\n",
      "unit_state        87.0\n",
      "act_general       85.0\n",
      "dtype: float64\n",
      "\n",
      "Top 10 bigrams for Project Safe Childhood :\n",
      "project_safe         472.0\n",
      "safe_childhood       472.0\n",
      "child_pornographi    445.0\n",
      "child_exploit        279.0\n",
      "sexual_exploit       223.0\n",
      "exploit_children     199.0\n",
      "plead_guilti         197.0\n",
      "exploit_obscen       175.0\n",
      "child_sexual         175.0\n",
      "obscen_section       174.0\n",
      "dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define a function to get the top 10 bigrams for each topic\n",
    "def get_top_bigrams(dtm, topic_column, topic_name):\n",
    "    # Subset the dtm to the specific topic\n",
    "    topic_dtm = dtm[dtm[topic_column] == topic_name]\n",
    "    # Sum the frequencies of each bigram\n",
    "    bigram_counts = topic_dtm.iloc[:, 3:].sum(axis=0)\n",
    "    # Get the top 10 bigrams\n",
    "    top_bigrams = bigram_counts.sort_values(ascending=False).head(10)\n",
    "    return top_bigrams\n",
    "\n",
    "# Print the top 10 bigrams for each of the three topics_clean\n",
    "for topic in ['Civil Rights', 'Hate Crimes', 'Project Safe Childhood']:\n",
    "    print(\"Top 10 bigrams for\", topic, \":\")\n",
    "    top_bigrams = get_top_bigrams(dtm_bigram, 'topics_clean', topic)\n",
    "    print(top_bigrams)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Optional extra credit (2 points)\n",
    "\n",
    "You notice that the pharmaceutical kickbacks press release we analyzed in question 1 was for an indictment, and that in the original data, there's not a clear label for whether a press release outlines an indictment (charging someone with a crime), a conviction (convicting them after that charge either via a settlement or trial), or a sentencing (how many years of prison or supervised release a defendant is sentenced to after their conviction).\n",
    "\n",
    "You want to see if you can identify pairs of press releases where one press release is from one stage (e.g., indictment) and another is from a different stage (e.g., a sentencing).\n",
    "\n",
    "You decide that one way to approach is to find the pairwise string similarity between each of the processed press releases in `doj_subset`. There are many ways to do this, so Google for some approaches, focusing on ones that work well for entire documents rather than small strings.\n",
    "\n",
    "Find the top two pairs (so four press releases total)-- do they seem like different stages of the same crime or just press releases covering similar crimes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
